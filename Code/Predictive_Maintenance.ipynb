{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"Incidents_imputed.xlsx\"\n",
    "GRAPH_PATH = \"Hetro_Final_NW_graph_1.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded incidents: 292829\n",
      "Loaded Graph Structure:\n",
      "HeteroData(\n",
      "  substation={\n",
      "    x=[347, 8],\n",
      "    node_ids=[347],\n",
      "  },\n",
      "  (substation, spatial, substation)={\n",
      "    edge_index=[2, 23378],\n",
      "    edge_attr=[23378, 8],\n",
      "  },\n",
      "  (substation, temporal, substation)={\n",
      "    edge_index=[2, 38188],\n",
      "    edge_attr=[38188, 2],\n",
      "  },\n",
      "  (substation, causal, substation)={\n",
      "    edge_index=[2, 10136],\n",
      "    edge_attr=[10136, 13],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "incident_df = pd.read_excel(DATA_PATH, parse_dates=['Job OFF Time', 'Job ON Time'])\n",
    "hetero_graph = torch.load(GRAPH_PATH)\n",
    "\n",
    "print(f\"Loaded incidents: {len(incident_df)}\")\n",
    "print(\"Loaded Graph Structure:\")\n",
    "print(hetero_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Metadata: <bound method HeteroData.metadata of HeteroData(\n",
      "  substation={\n",
      "    x=[347, 8],\n",
      "    node_ids=[347],\n",
      "  },\n",
      "  (substation, spatial, substation)={\n",
      "    edge_index=[2, 23378],\n",
      "    edge_attr=[23378, 8],\n",
      "  },\n",
      "  (substation, temporal, substation)={\n",
      "    edge_index=[2, 38188],\n",
      "    edge_attr=[38188, 2],\n",
      "  },\n",
      "  (substation, causal, substation)={\n",
      "    edge_index=[2, 10136],\n",
      "    edge_attr=[10136, 13],\n",
      "  }\n",
      ")>\n",
      "Substation Node IDs: ['7317:KONAWA OC PUMP', '7312:JUMPER CREEK', '7506:SASAKWA', '7412:PEARSON', '7508:EMAHAKA', '7417:TRIBBEY', '7307:LITTLE RIVER', '7410:MAUD TAP', '7505:WEWOKA', '7208:CYPRESS', '7429:MACOMB OC PUMP', '7306:FIXICO', '7321:LETHA', '7407:REMINGTON', '7405:SHAWNEE', '7430:INGLEWOOD', '7435:MISSION HILL', '7512:CROMWELL', '8617:SUNNYLANE', '8696:TINKER 6', '8687:TINKER FIELD 5', '8685:TINKER FIELD 4', '8697:TINKER FIELD 3', '8618:BARNES', '8686:GENERAL MOTORS', '7409:MCLOUD', '7411:DALE', '7436:WOLVERINE', '7437:MOBIL CHEMICAL', '7433:ROCK CREEK', '7432:ST GREGORY', '8471:NE 30TH ST', '8542:RENO', '8519:NE 10TH ST', '8522:MIDWAY', '8650:GLENDALE', '8662:SE 15TH ST', '8576:HORSESHOE LAKE', '7117:JACKTOWN', '7320:KOLACHE', '8463:CHITWOOD', '8411:ACORN', '8458:GREEN PASTURES', '7311:BURNETT', '7119:WARWICK', '7118:KEY WEST', '8113:CHEMETRON', '8158:SARA', '8155:WILL ROGERS', '8157:MACARTHUR', '8921:JENSEN RD', '8359:YUKON', '8360:WOODLAWN', '8224:XEROX', '8297:MORGAN ROAD', '8245:COUNCIL', '8299:WESTOAKS', '8266:MUSTANG STA', '8322:THIRTY EIGHTH', '8335:BETHANY', '8474:STONEWALL', '8220:MAY AVE', '8221:MERIDIAN', '8523:PARK PLACE NET', '8506:PARK PLACE', '8248:SW 5TH ST', '8206:ROBINSON AVE', '8905:EL RENO', '8323:RICHARDS', '8313:HAYMAKER', '8340:SILVER LAKE', '8381:DIVISION AVE', '8352:QUAIL CREEK', '8469:MEMORIAL', '8317:SKYLINE', '8336:TENNESSEE', '8464:BRYANT', '8365:LAKESIDE', '8347:WILSHIRE', '8493:KELLEY AVE', '8337:EIGHTY FOURTH', '8339:TULSA AVE', '8312:BELLE ISLE STA', '8434:DEEP FORK', '8430:REMINGTON PARK', '8321:PIEDMONT', '8342:BRADEN PARK', '8308:LONE OAK', '8398:CHISHOLM CREEK', '8910:OKARCHE', '8822:WATERLOO', '8908:ROMAN NOSE', '8807:CRESCENT', '8806:COTTONWOOD CR', '8814:PINE STREET', '8813:FITZGERALD CR', '8907:CANTON', '8904:CANTON LAKE', '8906:SOUTHARD', '8810:ORLANDO', '5620:PRAIRIE POINT', '5608:SHELL ELMORE C', '3315:WARNER TAP', '5623:SUN OIL', '5621:CHIGLEY', '5605:PAULS VALLEY', '5619:RUSH CREEK', '5611:MAYSVILLE', '5607:ROSEDALE TAP', '5607:ROSE DALE TAP', '5606:WALNUT CREEK', '8721:SPRING HILL', '8708:CEDAR LANE', '8726:NOBLE', '8715:ETOWAH', '8727:STUBBEMAN', '8705:NORMAN', '8724:WHOUSE NORMAN', '8728:CHERRY CREEK', '8706:BOYD', '8722:MILLENNIUM', '8709:LITTLE R LAKE', '8707:WILKINSON', '8714:LITTLE AXE', '8194:WESTMOORE', '8163:SOUTHGATE', '8654:MOORE', '8717:TURNER', '8719:PLEASANT VLY', '8723:INDIAN HILL', '8680:ELM CREEK', '8621:DRAPER LAKE', '8711:TEXAS PL STELL', '8132:KENTUCKY', '8141:SAGE', '8129:SW 64TH ST', '8151:PENNSYLVANIA', '8170:SANTA FE AVE', '8656:FOSTER', '8658:DISCOVERY', '8690:WILD MARY', '8115:SW 22ND ST', '8620:LIGHTNING CR', '8626:TROSPER', '8535:OU MED CENTER', '8888:EDMOND DIST', '9223:SUBIACO', '9965:NORTON/ALCOA', '7625:SEAWAY', '5117:MARIETTA', '5408:CANEY CREEK', '5506:BODLE', '5505:DURANT', '5504:BUTTERFIELD', '5511:BOKCHITO', '5410:GLASSES', '5409:LITTLE CITY', '5508:BROWN OG AND E', '5312:SINCL PL RINGL', '5122:DILLARD', '5108:WOLF CREEK', '5125:LONE GROVE', '5123:UNIROYAL', '5107:TOWER HEIGHTS', '5104:FOUNDATION', '5105:ARDMORE', '5907:RUSSETT', '5510:EXPL PL DURANT', '5305:HEALDTON', '5322:DUNDEE', '5129:TOTAL PETRO', '5106:HARRIS ST', '5906:TISHOMINGO', '5319:FOX', '5124:BERWYN', '5306:COUNTY LINE', '5309:POOLEVILLE', '5709:PRICES FALLS', '5706:LAKE ARBUCKLE', '5707:MILL CREEK', '5321:WILDHORSE', '5311:RATLIFF', '5708:DAVIS', '5712:JOLLYVILLE', '5705:SULPHUR', '5819:BLUE RIVER', '5808:HARDEN CITY', '5816:PARK LANE', '5807:AHLOSO', '5809:VANOSS', '5806:VALLEY VIEW', '5813:BYNG SPA', '4461:TURKEY CREEK', '4624:IODINE', '4611:DEWEY', '4414:HENNESSEY', '4616:NEWMAN AVE', '4109:WAUKOMIS', '4353:OTTER', '4608:WOODWARD DISTR', '4609:GLASS MTNS', '4519:CLEO', '4518:MENO TAP', '4106:HEMLOCK', '4112:CHESTNUT', '4107:CLEVELAND AVE', '4153:IMO', '4105:ENID', '4151:VANCE AFB', '4135:SO 4TH ST', '4158:FAIRMONT', '4623:TANGIER', '4621:WOODWARD NITRO', '4606:CEDAR AVE', '4157:NE ENID', '4239:OTOE', '4522:ALINE', '4538:GOLTRY TAP', '4123:KREMLIN TAP', '4219:BUNCH CREEK', '4528:SALINE', '4225:FOUR CORNERS', '4230:THREE SANDS', '4261:WHITE EAGLE', '4242:CONT BLACKS', '4215:CONT EMPIRE', '4215:CONTINENTAL EMPIRE', '4246:CHEROKEE PL PO', '4265:STANDING BEAR', '4705:ALVA', '4707:KNOBHILL', '4220:CLYDE', '4210:MEDFORD', '4240:DEER CREEK', '4256:SINCL BLKWELL', '3507:HOWE', '3514:HEAVENER', '3505:POTEAU', '3516:TARBY', '3506:CAVANAL MTN', '3509:PANAMA', '3508:SPIRO COAL', '9138:ARKOMA', '9112:BELLE AVE', '3907:YAFFE', '9109:SOUTHSIDE', '9113:CAVANAUGH', '9137:PARK VIEW', '9111:MASSARD', '9980:QUANEX', '9125:CARNALL', '9140:COLONY', '9115:BATTLEFIELD', '9117:BARLING', '9217:LAVACA BLOOMER', '9215:CHARLESTON', '9224:BRANCH', '9230:SHORT MTN', '5812:SALT CREEK', '3608:MULDROW', '3607:ROLAND ROAD', '9126:OAK PARK', '9135:GERBER ROAD', '9508:WHITESIDE', '9114:JOHNSON', '9410:IGO', '9318:ALMA', '9330:SIMMONS', '9406:HELBERG', '9412:ALTUS', '9418:LITTLE SPADRA', '3332:EUFAULA SPA', '3325:WELLS', '3314:PORUM', '3312:CHECOTAH', '3316:WARNER', '3313:ILLINOIS RIVER', '3335:KERR MCGEE SEQ', '3327:VIAN', '3111:FIVE TRIBES', '3128:HANCOCK', '7706:BRISTOW', '3218:BEGGS', '3326:JAMESVILLE', '3137:AGENCY', '3109:HONOR HEIGHTS', '3136:EUCLID', '3131:MUSKOGEE WW', '3132:MUSKOGEE PORT', '3321:ROSS LAKE', '3114:TENNYSON', '3116:CALLERY', '3110:RIVERSIDE', '3209:BIXBY', '7614:KNIPE', '7609:SHELL PL CUSH', '7632:OAK GROVE', '7628:PIPELINE', '7626:SKELLY PL CUSH', '7693:ARCO PL CUSHIN', '7605:DRUMRIGHT', '7608:TIGER CREEK', '7636:GREAT PLAINS', '3205:SAPULPA', '9104:J STREET', '3220:KELLYVILLE', '3214:LONE STAR', '3216:BEELINE', '3207:HILL TOP', '7606:CUSHING TAP', '7610:PRINCEVILLE', '3213:BOWDEN', '3210:HICKORY HILL', '3217:POLECAT CREEK', '7633:MCELROY', '7620:ANTIOCH', '7631:JENNINGS', '7611:MORRISON TAP', '3208:TIBBENS ROAD', '9413:RAZORBACK', '5512:COLBERT', '4119:GLENWOOD', '9340:TWIN BRIDGES', '8222:CZECH HALL', '8361:WESTERN AVE', '5407:MADILL INDUSTR', '8416:DUNJEE', '8150:HOBBY LOBBY', '8209:CLASSEN', '7104:BELLCOW', '9512:ADABELL ROAD', '5109:ARDMORE WEST', '5515:BLUE BIRD', '4354:BRONCO ROAD', '7629:CUSHING OILFLD', '4155:ENID INDUSTRIA', '8819:LIBERTY LAKE', '8924:TALL BEAR', '4211:GRANT COUNTY', '4241:DEADMAN CREEK', '8536:WASHINGTON PAR', '8536:WASHINGTON PARK', '7316:YOUNG CREEK', '8692:AIR DEPOT', '4159:PLAINSMEN', '5804:SOUTH ADA', '5507:REBAR']\n"
     ]
    }
   ],
   "source": [
    "# Check if node IDs are stored in metadata\n",
    "print(\"Graph Metadata:\", hetero_graph.metadata)\n",
    "\n",
    "# Check if there's a dictionary of node mappings\n",
    "if hasattr(hetero_graph['substation'], 'node_ids'):\n",
    "    print(\"Substation Node IDs:\", hetero_graph['substation'].node_ids)\n",
    "elif hasattr(hetero_graph['substation'], 'node_names'):\n",
    "    print(\"Substation Node Names:\", hetero_graph['substation'].node_names)\n",
    "elif hasattr(hetero_graph['substation'], 'mapping'):\n",
    "    print(\"Substation Mapping:\", hetero_graph['substation'].mapping)\n",
    "else:\n",
    "    print(\"‚ùå No explicit substation node IDs found in metadata!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2 invalid substations due to format issues before processing.\n",
      "Final unique substations flagged: 178 out of 362 total substations.\n",
      "\n",
      "Substations with NaN in days_until_next_incident:\n",
      "10648\n",
      "\n",
      "Substations with days_until_next_incident > 180:\n",
      "15987\n",
      "\n",
      "Final Target Distribution After Fix:\n",
      "needs_replacement\n",
      "0    184\n",
      "1    178\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_target_variable(df, severe_causes=None, critical_equipment=None, return_thresholds=False):\n",
    "    \"\"\"\n",
    "    Creates target labels using a 180-day window with cleaned substation names.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 Clean and standardize Job Substation names (Embedded)\n",
    "    def clean_substation_name(x):\n",
    "        match = re.match(r'(\\d+)\\s*:\\s*(.+)', str(x).strip().upper())\n",
    "        return f\"{match.group(1)}:{match.group(2).strip()}\" if match else None\n",
    "\n",
    "    df['Job Substation'] = df['Job Substation'].astype(str).apply(clean_substation_name)\n",
    "\n",
    "    # Drop invalid substations (Ensuring before processing)\n",
    "    dropped_count = df['Job Substation'].isna().sum()\n",
    "    df.dropna(subset=['Job Substation'], inplace=True)\n",
    "\n",
    "    # 2 Compute Cause Thresholds\n",
    "    cause_stats = df.groupby('Cause Desc').agg(\n",
    "        median_duration=('Job Duration Mins', 'median'),\n",
    "        max_customers=('Custs Affected', 'max')\n",
    "    )\n",
    "    cause_thresholds = cause_stats.quantile(0.90)\n",
    "    \n",
    "    # 3 Identify Severe Causes\n",
    "    if severe_causes is None:\n",
    "        severe_causes = cause_stats[\n",
    "            (cause_stats['median_duration'] > cause_thresholds['median_duration']) &\n",
    "            (cause_stats['max_customers'] > cause_thresholds['max_customers'])\n",
    "        ].index.unique()\n",
    "    \n",
    "    # 4 Identify Critical Equipment\n",
    "    equip_stats = df.groupby('Equip Desc').agg(\n",
    "        failure_freq=('Job Display ID', 'count'),\n",
    "        saidi=('Job SAIDI', 'mean')\n",
    "    )\n",
    "    equipment_thresholds = equip_stats.quantile(0.90)\n",
    "    \n",
    "    if critical_equipment is None:\n",
    "        critical_equipment = df.groupby('Equip Desc').filter(\n",
    "            lambda x: (len(x) > equipment_thresholds['failure_freq']) and\n",
    "                      (x['Job SAIDI'].mean() > equipment_thresholds['saidi'])\n",
    "        )['Equip Desc'].unique()\n",
    "    \n",
    "    # 5 Sorting and Computing Intervals\n",
    "    df_sorted = df.sort_values(['Job Substation', 'Equip Desc', 'Job OFF Time']).copy()\n",
    "    df_sorted['days_until_next_incident'] = df_sorted.groupby(\n",
    "        ['Job Substation', 'Equip Desc']\n",
    "    )['Job OFF Time'].diff(-1).dt.days.abs()\n",
    "    \n",
    "    # 6 Assign `needs_replacement` flag\n",
    "    df_sorted['needs_replacement'] = np.where(\n",
    "        (df_sorted['Cause Desc'].isin(severe_causes)) & \n",
    "        (df_sorted['Equip Desc'].isin(critical_equipment)) & \n",
    "        ((df_sorted['days_until_next_incident'] > 180) | \n",
    "         df_sorted['days_until_next_incident'].isna()),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # 7 Final Aggregation using Cleaned Substations\n",
    "    final_targets = df_sorted.groupby('Job Substation')['needs_replacement'].max()\n",
    "\n",
    "    # Debugging Info\n",
    "    print(f\"Dropped {dropped_count} invalid substations due to format issues before processing.\")\n",
    "    print(f\"Final unique substations flagged: {final_targets.sum()} out of {final_targets.count()} total substations.\")\n",
    "\n",
    "    # Return results\n",
    "    if return_thresholds:\n",
    "        return final_targets, severe_causes, critical_equipment, df_sorted\n",
    "    else:\n",
    "        return final_targets, df_sorted\n",
    "\n",
    "# Run the function using the loaded incident data\n",
    "full_targets, filtered_incidents = create_target_variable(incident_df, return_thresholds=False)\n",
    "\n",
    "# Debugging: Check NaN values in `days_until_next_incident`\n",
    "print(\"\\nSubstations with NaN in days_until_next_incident:\")\n",
    "print(filtered_incidents['days_until_next_incident'].isna().sum())\n",
    "\n",
    "print(\"\\nSubstations with days_until_next_incident > 180:\")\n",
    "print((filtered_incidents['days_until_next_incident'] > 180).sum())\n",
    "\n",
    "print(\"\\nFinal Target Distribution After Fix:\")\n",
    "print(full_targets.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Warning: 15 substations in target are missing from graph.\n",
      "Examples: ['8168:CEMETERY RD', '3329:KEETOOWAH', '4233:COWBOY HILL', '3117:THREE RIVERS', '8417:ROUND BARN']\n",
      "\n",
      " Targets integrated successfully!\n",
      "Graph node count: 347\n",
      "Target distribution:\n",
      "needs_replacement\n",
      "0    177\n",
      "1    170\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def integrate_target_variable(graph, target_variable):\n",
    "    \"\"\"\n",
    "    Integrates the computed target labels into the graph structure safely.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 Extract substation names from graph metadata\n",
    "    substation_names = graph['substation'].node_ids  \n",
    "\n",
    "    # 2 Validate alignment (Check if all substations exist in both)\n",
    "    graph_subs = set(substation_names)\n",
    "    target_subs = set(target_variable.index)\n",
    "\n",
    "    missing_from_graph = target_subs - graph_subs\n",
    "    missing_from_target = graph_subs - target_subs\n",
    "\n",
    "    if missing_from_graph:\n",
    "        print(f\"‚ö† Warning: {len(missing_from_graph)} substations in target are missing from graph.\")\n",
    "        print(\"Examples:\", list(missing_from_graph)[:5])\n",
    "\n",
    "    if missing_from_target:\n",
    "        print(f\"‚ö† Warning: {len(missing_from_target)} substations in graph are missing from target.\")\n",
    "        print(\"Examples:\", list(missing_from_target)[:5])\n",
    "\n",
    "    # 3 Align using the graph's node order (Ensuring safe reindexing)\n",
    "    target_variable = target_variable.reindex(substation_names).fillna(0)  # Fill missing targets with 0\n",
    "\n",
    "    # 4 Convert to tensor and integrate into graph\n",
    "    graph['substation'].y = torch.tensor(\n",
    "        target_variable.values.astype(np.float32), dtype=torch.float\n",
    "    )\n",
    "\n",
    "    # 5 Final integrity check\n",
    "    assert len(graph['substation'].y) == len(substation_names), \"Target tensor size mismatch with graph nodes!\"\n",
    "\n",
    "    print(\"\\n Targets integrated successfully!\")\n",
    "    print(f\"Graph node count: {len(substation_names)}\")\n",
    "    print(f\"Target distribution:\\n{target_variable.value_counts()}\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Usage: Integrate targets into the loaded graph\n",
    "hetero_graph = integrate_target_variable(\n",
    "    graph=hetero_graph,\n",
    "    target_variable=full_targets  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incidents span: 2015-01-01 to 2021-12-31\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncidents span: \u001b[39m\u001b[39m{\u001b[39;00mstart_date\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mend_date\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Plot monthly incident counts\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m4\u001b[39m))\n\u001b[1;32m      8\u001b[0m incident_df\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mJob OFF Time\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mJob Display ID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcount()\u001b[39m.\u001b[39mplot()\n\u001b[1;32m      9\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mIncident Frequency Over Time\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Get incident date range\n",
    "start_date = incident_df['Job OFF Time'].min().strftime('%Y-%m-%d')\n",
    "end_date = incident_df['Job OFF Time'].max().strftime('%Y-%m-%d')\n",
    "print(f\"Incidents span: {start_date} to {end_date}\")\n",
    "\n",
    "# Plot monthly incident counts\n",
    "plt.figure(figsize=(12,4))\n",
    "incident_df.set_index('Job OFF Time').resample('M')['Job Display ID'].count().plot()\n",
    "plt.title(\"Incident Frequency Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"# Incidents\")\n",
    "plt.axvline(pd.Timestamp('2022-06-01'), color='red', linestyle='--', label='Proposed Split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 206, Val: 70, Test: 71\n"
     ]
    }
   ],
   "source": [
    "# 1 Calculate label determination date for each substation\n",
    "label_dates = incident_df.groupby('Job Substation')['Job OFF Time'].max() + pd.Timedelta(days=180)\n",
    "\n",
    "# 2 Compute split dates (Train 60% / Val 20% / Test 20%)\n",
    "split_train = label_dates.quantile(0.60)  # Train: 60%\n",
    "split_val = label_dates.quantile(0.80)    # Validation: 20%, Test: 20%\n",
    "\n",
    "# 3 Assign substations to train, validation, and test sets\n",
    "train_subs = label_dates[label_dates <= split_train].index\n",
    "val_subs = label_dates[(label_dates > split_train) & (label_dates <= split_val)].index\n",
    "test_subs = label_dates[label_dates > split_val].index\n",
    "\n",
    "# 4 Apply masks to the graph (Ensure `val_mask` is included)\n",
    "hetero_graph['substation'].train_mask = torch.tensor(\n",
    "    [n in train_subs for n in hetero_graph['substation'].node_ids], dtype=torch.bool\n",
    ")\n",
    "hetero_graph['substation'].val_mask = torch.tensor(  \n",
    "    [n in val_subs for n in hetero_graph['substation'].node_ids], dtype=torch.bool\n",
    ")\n",
    "hetero_graph['substation'].test_mask = torch.tensor(\n",
    "    [n in test_subs for n in hetero_graph['substation'].node_ids], dtype=torch.bool\n",
    ")\n",
    "\n",
    "# 5 Verify split distribution\n",
    "train_count = hetero_graph['substation'].train_mask.sum().item()\n",
    "val_count = hetero_graph['substation'].val_mask.sum().item()\n",
    "test_count = hetero_graph['substation'].test_mask.sum().item()\n",
    "\n",
    "print(f\" Train: {train_count}, Val: {val_count}, Test: {test_count}\")\n",
    "if train_count + val_count + test_count != len(hetero_graph['substation'].node_ids):\n",
    "    print(f\"‚ö† Warning: Some nodes may be missing from the split!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedGCNConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, edge_attr_dim):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, out_channels, add_self_loops=False)\n",
    "        \n",
    "        # Dynamically infer edge_attr_dim at runtime\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_attr_dim, 64),  # Adjusted dynamically\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(out_channels)\n",
    "        self.residual_proj = nn.Linear(in_channels, out_channels) if in_channels != out_channels else nn.Identity()\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.edge_mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        if edge_index.size(1) == 0:\n",
    "            return self.residual_proj(x)  # Handle empty graphs safely\n",
    "\n",
    "        #  Dynamically check edge attributes\n",
    "        edge_dim = edge_attr.shape[1] if edge_attr.dim() > 1 else 1\n",
    "\n",
    "        if edge_dim != self.edge_mlp[0].in_features:\n",
    "            print(f\"‚ö† Adjusting edge MLP: Expected {self.edge_mlp[0].in_features}, got {edge_dim}\")\n",
    "            self.edge_mlp[0] = nn.Linear(edge_dim, 64).to(edge_attr.device)\n",
    "\n",
    "        # Normalize edge attributes\n",
    "        edge_attr = (edge_attr - edge_attr.mean(dim=0)) / (edge_attr.std(dim=0) + 1e-8)\n",
    "        edge_attr = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "            \n",
    "        # Compute edge weights\n",
    "        weights = self.edge_mlp(edge_attr).squeeze()\n",
    "\n",
    "        # Apply GCN with weighted edges\n",
    "        out = self.gcn(x, edge_index, edge_weight=weights + 1e-8)\n",
    "\n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out)\n",
    "\n",
    "        return out + self.residual_proj(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PowerGridGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, edge_dims, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.edge_types = list(edge_dims.keys())  # Dynamic edge types\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_dim = in_channels\n",
    "        \n",
    "        # Validate edge dimensions\n",
    "        assert set(edge_dims.keys()) == set(self.edge_types), \"Mismatch in provided edge dimensions!\"\n",
    "        \n",
    "        # Initialize GCN layers\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.ModuleDict({\n",
    "                et: WeightedGCNConv(current_dim, hidden_dim, edge_dims[et])\n",
    "                for et in self.edge_types\n",
    "            }))\n",
    "            current_dim = hidden_dim  # Maintain consistency across layers\n",
    "            \n",
    "        # Prediction head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Learnable aggregation across edge types\n",
    "        self.aggregation_weights = nn.Parameter(torch.ones(len(self.edge_types)))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Apply proper initialization to all layers\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\n",
    "    def _register_edge_stats(self, data):\n",
    "        \"\"\"Compute edge feature statistics from data and store them for normalization\"\"\"\n",
    "        for et in self.edge_types:\n",
    "            attr = data['substation', et, 'substation'].edge_attr\n",
    "            self.register_buffer(f'{et}_mean', attr.mean(dim=0))\n",
    "            self.register_buffer(f'{et}_std', attr.std(dim=0) + 1e-8)  # Prevent division by zero\n",
    "\n",
    "    def forward(self, data, mode='train'):\n",
    "        x = data['substation'].x\n",
    "        edge_masks = self._get_edge_masks(data, mode)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            messages = []\n",
    "            for et in self.edge_types:\n",
    "                edge_data = data['substation', et, 'substation']\n",
    "                idx, attr = edge_data.edge_index, edge_data.edge_attr\n",
    "                \n",
    "                # Apply strict edge masking\n",
    "                mask = edge_masks[et]\n",
    "                idx, attr = idx[:, mask], attr[mask]\n",
    "                \n",
    "                # Handle empty edges\n",
    "                if idx.shape[1] == 0:\n",
    "                    messages.append(x.new_zeros(x.size(0), layer[et].gcn.out_channels))\n",
    "                else:\n",
    "                    # Edge Normalization\n",
    "                    attr = (attr - getattr(self, f'{et}_mean')) / getattr(self, f'{et}_std')\n",
    "                    messages.append(layer[et](x, idx, attr))\n",
    "\n",
    "            # Learnable weighted aggregation across edge types\n",
    "            x = torch.stack(messages, dim=0)  # Shape: (num_edge_types, num_nodes, hidden_dim)\n",
    "            x = torch.sum(x * F.softmax(self.aggregation_weights, dim=0)[:, None, None], dim=0)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        return self.head(x)\n",
    "\n",
    "    def _get_edge_masks(self, data, mode='train'):\n",
    "        \"\"\"Create edge masks to strictly isolate train/test splits\"\"\"\n",
    "        masks = {}\n",
    "        split_mask = getattr(data['substation'], f\"{mode}_mask\").bool().to(data['substation'].x.device)\n",
    "\n",
    "        for et in self.edge_types:\n",
    "            edge_info = data['substation', et, 'substation']\n",
    "            edge_index = edge_info.edge_index\n",
    "            \n",
    "            # Strict isolation: both nodes must be in the current split\n",
    "            source_mask = split_mask[edge_index[0]]\n",
    "            target_mask = split_mask[edge_index[1]]\n",
    "            masks[et] = source_mask & target_mask  # Ensures strict split enforcement\n",
    "        \n",
    "        return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.2212 | Val Loss: 0.6851 | LR: 1.00e-03\n",
      "Epoch 10 | Train Loss: 0.1994 | Val Loss: 0.6700 | LR: 1.00e-03\n",
      "Epoch 20 | Train Loss: 0.1854 | Val Loss: 0.6690 | LR: 1.00e-03\n",
      "Epoch 30 | Train Loss: 0.1704 | Val Loss: 0.6678 | LR: 1.00e-03\n",
      "Epoch 40 | Train Loss: 0.1684 | Val Loss: 0.6648 | LR: 1.00e-03\n",
      "Epoch 50 | Train Loss: 0.1601 | Val Loss: 0.6634 | LR: 1.00e-03\n",
      "Epoch 60 | Train Loss: 0.1469 | Val Loss: 0.6605 | LR: 1.00e-03\n",
      "Epoch 70 | Train Loss: 0.1448 | Val Loss: 0.6582 | LR: 1.00e-03\n",
      "Epoch 80 | Train Loss: 0.1398 | Val Loss: 0.6563 | LR: 1.00e-03\n",
      "Epoch 90 | Train Loss: 0.1417 | Val Loss: 0.6537 | LR: 1.00e-03\n",
      "Epoch 100 | Train Loss: 0.1383 | Val Loss: 0.6511 | LR: 1.00e-03\n",
      "Epoch 110 | Train Loss: 0.1397 | Val Loss: 0.6480 | LR: 1.00e-03\n",
      "Epoch 120 | Train Loss: 0.1333 | Val Loss: 0.6453 | LR: 1.00e-03\n",
      "Epoch 130 | Train Loss: 0.1336 | Val Loss: 0.6430 | LR: 1.00e-03\n",
      "Epoch 140 | Train Loss: 0.1334 | Val Loss: 0.6415 | LR: 1.00e-03\n",
      "Epoch 150 | Train Loss: 0.1288 | Val Loss: 0.6384 | LR: 1.00e-03\n",
      "Epoch 160 | Train Loss: 0.1272 | Val Loss: 0.6351 | LR: 1.00e-03\n",
      "Epoch 170 | Train Loss: 0.1322 | Val Loss: 0.6330 | LR: 1.00e-03\n",
      "Epoch 180 | Train Loss: 0.1290 | Val Loss: 0.6313 | LR: 1.00e-03\n",
      "Epoch 190 | Train Loss: 0.1256 | Val Loss: 0.6274 | LR: 1.00e-03\n",
      "Epoch 200 | Train Loss: 0.1256 | Val Loss: 0.6249 | LR: 1.00e-03\n",
      "\n",
      " Final Test Metrics:\n",
      "Accuracy: 0.8310 | Precision: 0.8364\n",
      "Recall: 0.9388 | F1: 0.8846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss to handle class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.75, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * BCE_loss).mean()\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hetero_graph = hetero_graph.to(device)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "in_channels = hetero_graph['substation'].x.size(1)\n",
    "hidden_channels = 128\n",
    "edge_attr_dims = {'spatial': 8, 'temporal': 2, 'causal': 13}\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "\n",
    "# --- Model & Optimizer ---\n",
    "model = PowerGridGNN(\n",
    "    in_channels=in_channels,\n",
    "    hidden_dim=hidden_channels,\n",
    "    edge_dims=edge_attr_dims,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# Register edge stats for normalization (if needed)\n",
    "model._register_edge_stats(hetero_graph)  \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "\n",
    "# --- Training Loop ---\n",
    "best_val_loss = float('inf')\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(hetero_graph, mode='train')\n",
    "    loss = criterion(out[hetero_graph['substation'].train_mask].squeeze(),\n",
    "                     hetero_graph['substation'].y[hetero_graph['substation'].train_mask].squeeze().float())\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Prevents exploding gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_val = model(hetero_graph, mode='val')\n",
    "        val_loss = F.binary_cross_entropy_with_logits(\n",
    "            out_val[hetero_graph['substation'].val_mask].squeeze(),\n",
    "            hetero_graph['substation'].y[hetero_graph['substation'].val_mask].squeeze().float()\n",
    "        )\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # --- Early stopping ---\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    \n",
    "    # --- Logging ---\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    if no_improve >= patience:\n",
    "        print(\"üõë Early stopping!\")\n",
    "        break\n",
    "\n",
    "# --- Final Test ---\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out_test = model(hetero_graph, mode='test')\n",
    "    probs = torch.sigmoid(out_test[hetero_graph['substation'].test_mask].squeeze())\n",
    "    predictions = (probs > 0.6).float()\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    y_true = hetero_graph['substation'].y[hetero_graph['substation'].test_mask].cpu()\n",
    "    y_pred = predictions.cpu()\n",
    "    \n",
    "    acc = (y_true == y_pred).float().mean()\n",
    "    precision = (y_true * y_pred).sum() / (y_pred.sum() + 1e-8)\n",
    "    recall = (y_true * y_pred).sum() / (y_true.sum() + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    \n",
    "    print(\"\\n Final Test Metrics:\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f} | F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing: lr=0.001, hidden_dim=128, num_layers=2, wd=0.0001\n",
      " Result: best_val_loss = 0.6544\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=128, num_layers=2, wd=1e-05\n",
      " Result: best_val_loss = 0.6639\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=128, num_layers=3, wd=0.0001\n",
      " Result: best_val_loss = 0.6438\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=128, num_layers=3, wd=1e-05\n",
      " Result: best_val_loss = 0.6506\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=256, num_layers=2, wd=0.0001\n",
      " Result: best_val_loss = 0.6332\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=256, num_layers=2, wd=1e-05\n",
      " Result: best_val_loss = 0.6389\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=256, num_layers=3, wd=0.0001\n",
      " Result: best_val_loss = 0.6369\n",
      "\n",
      " Testing: lr=0.001, hidden_dim=256, num_layers=3, wd=1e-05\n",
      " Result: best_val_loss = 0.6358\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=128, num_layers=2, wd=0.0001\n",
      " Result: best_val_loss = 0.6653\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=128, num_layers=2, wd=1e-05\n",
      " Result: best_val_loss = 0.6664\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=128, num_layers=3, wd=0.0001\n",
      " Result: best_val_loss = 0.6636\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=128, num_layers=3, wd=1e-05\n",
      " Result: best_val_loss = 0.6632\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=256, num_layers=2, wd=0.0001\n",
      " Result: best_val_loss = 0.6471\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=256, num_layers=2, wd=1e-05\n",
      " Early stopping at epoch 35 (Best Val Loss: 0.6686)\n",
      " Result: best_val_loss = 0.6686\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=256, num_layers=3, wd=0.0001\n",
      " Result: best_val_loss = 0.6519\n",
      "\n",
      " Testing: lr=0.0005, hidden_dim=256, num_layers=3, wd=1e-05\n",
      " Early stopping at epoch 21 (Best Val Loss: 0.6630)\n",
      " Result: best_val_loss = 0.6630\n",
      "\n",
      "\n",
      " Best Hyperparameter Configuration:\n",
      "{'lr': 0.001, 'hidden_dim': 256, 'num_layers': 2, 'wd': 0.0001, 'val_loss': 0.6331619024276733}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- Define a function to train and validate the model ---\n",
    "def train_and_validate(model, data, criterion, optimizer, scheduler, \n",
    "                       train_mask, val_mask, device, epochs=100, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #  Training Step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, mode='train')\n",
    "        loss = criterion(\n",
    "            out[train_mask].squeeze(),\n",
    "            data['substation'].y[train_mask].squeeze().float()\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        #  Validation Step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out_val = model(data, mode='val')\n",
    "            val_loss = F.binary_cross_entropy_with_logits(\n",
    "                out_val[val_mask].squeeze(),\n",
    "                data['substation'].y[val_mask].squeeze().float()\n",
    "            )\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        #  Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            print(f\" Early stopping at epoch {epoch} (Best Val Loss: {best_val_loss:.4f})\")\n",
    "            break\n",
    "\n",
    "    return best_val_loss.item()\n",
    "\n",
    "# --- Hyperparameter grid ---\n",
    "learning_rates = [0.001, 0.0005]\n",
    "hidden_dims = [128, 256]\n",
    "num_layers_list = [2, 3]\n",
    "weight_decays = [1e-4, 1e-5]\n",
    "\n",
    "results = []\n",
    "\n",
    "#  Hyperparameter tuning loop\n",
    "for lr, hidden_dim, num_layers, wd in itertools.product(learning_rates, hidden_dims, num_layers_list, weight_decays):\n",
    "    print(f\" Testing: lr={lr}, hidden_dim={hidden_dim}, num_layers={num_layers}, wd={wd}\")\n",
    "    \n",
    "    # Instantiate a fresh model for each run\n",
    "    model = PowerGridGNN(\n",
    "        in_channels=hetero_graph['substation'].x.size(1),\n",
    "        hidden_dim=hidden_dim,\n",
    "        edge_dims=edge_attr_dims,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    model._register_edge_stats(hetero_graph)  # Edge normalization\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "    criterion = FocalLoss(alpha=0.75, gamma=2)\n",
    "    \n",
    "    # Train and Validate\n",
    "    best_val_loss = train_and_validate(\n",
    "        model, \n",
    "        hetero_graph, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        hetero_graph['substation'].train_mask, \n",
    "        hetero_graph['substation'].val_mask, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'wd': wd,\n",
    "        'val_loss': best_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\" Result: best_val_loss = {best_val_loss:.4f}\\n\")\n",
    "\n",
    "# --- üîπ Print Best Hyperparameter Configuration ---\n",
    "best_config = min(results, key=lambda x: x['val_loss'])\n",
    "print(\"\\n Best Hyperparameter Configuration:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final Model Training Completed. Best Val Loss: 0.6159\n"
     ]
    }
   ],
   "source": [
    "# --- Load Best Hyperparameters ---\n",
    "best_params = {'lr': 0.001, 'hidden_dim': 256, 'num_layers': 2, 'wd': 0.0001}\n",
    "\n",
    "# --- Initialize Model ---\n",
    "best_model = PowerGridGNN(\n",
    "    in_channels=hetero_graph['substation'].x.size(1),\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    edge_dims=edge_attr_dims,\n",
    "    num_layers=best_params['num_layers']\n",
    ").to(device)\n",
    "best_model._register_edge_stats(hetero_graph)  # Normalize edge attributes\n",
    "\n",
    "# --- Define Optimizer & Loss ---\n",
    "optimizer = torch.optim.AdamW(best_model.parameters(), lr=best_params['lr'], weight_decay=best_params['wd'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2)  # Adjust if needed\n",
    "\n",
    "# --- Train the Best Model ---\n",
    "final_val_loss = train_and_validate(\n",
    "    best_model, \n",
    "    hetero_graph, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    hetero_graph['substation'].train_mask, \n",
    "    hetero_graph['substation'].val_mask, \n",
    "    device, \n",
    "    epochs=200, \n",
    "    patience=20\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Final Model Training Completed. Best Val Loss: {final_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **Final Test Metrics**:\n",
      " Accuracy: 0.6901\n",
      " Precision: 0.6901\n",
      " Recall: 1.0000\n",
      " F1 Score: 0.8167\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Load Best Model Weights ---\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    out_test = best_model(hetero_graph, mode='test')\n",
    "    probs = torch.sigmoid(out_test[hetero_graph['substation'].test_mask].squeeze())\n",
    "    predictions = (probs > 0.5).float()\n",
    "\n",
    "# --- Extract Ground Truth ---\n",
    "y_true = hetero_graph['substation'].y[hetero_graph['substation'].test_mask].cpu()\n",
    "y_pred = predictions.cpu()\n",
    "\n",
    "# --- Compute Metrics ---\n",
    "acc = (y_true == y_pred).float().mean()\n",
    "precision = (y_true * y_pred).sum() / (y_pred.sum() + 1e-8)\n",
    "recall = (y_true * y_pred).sum() / (y_true.sum() + 1e-8)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "# --- Print Metrics ---\n",
    "print(\"\\n **Final Test Metrics**:\")\n",
    "print(f\" Accuracy: {acc:.4f}\")\n",
    "print(f\" Precision: {precision:.4f}\")\n",
    "print(f\" Recall: {recall:.4f}\")\n",
    "print(f\" F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
