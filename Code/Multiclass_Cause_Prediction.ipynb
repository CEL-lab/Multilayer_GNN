{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and Graph Loaded Successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.145354.bright04/ipykernel_2184525/672223291.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hetero_graph = torch.load(GRAPH_PATH)\n"
     ]
    }
   ],
   "source": [
    "# Cell 01 : Loading \n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.nn import HeteroConv, RGCNConv, GATConv, Linear\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"Incidents_imputed.xlsx\"\n",
    "GRAPH_PATH = \"Hetro_Final_NW_graph_1.pt\"\n",
    "\n",
    "# Load incident dataset\n",
    "incident_df = pd.read_excel(DATA_PATH, parse_dates=['Job OFF Time', 'Job ON Time'], engine='openpyxl')\n",
    "\n",
    "# Load heterogeneous graph\n",
    "hetero_graph = torch.load(GRAPH_PATH)\n",
    "\n",
    "print(\"Data and Graph Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 'Equip Desc' entries: 112\n",
      "Sample 'Equip Desc' values:\n",
      "['CONDUCTOR SECONDARY - OH' 'CONDUCTOR SERVICE - OH'\n",
      " 'CONDUCTOR PRIMARY - UG' 'FUSE - TRANSFORMER' 'CONNECTOR'\n",
      " 'CUSTOMER EQUIPMENT' 'FUSE - PRIMARY' 'CONDUCTOR PRIMARY - OH' 'CUTOUT'\n",
      " 'CONDUCTOR SECONDARY - UG' 'GROUNDING' 'FUSE BARREL' 'TRANSFORMER OH'\n",
      " 'INSULATOR' 'POLE' 'ARRESTOR OH' 'CROSSARM' 'JUMPER' 'SUBSTATION CIRCUIT'\n",
      " 'CAPACITOR']\n",
      "\n",
      "Equipment Description Counts:\n",
      "Equip Desc\n",
      "FUSE - TRANSFORMER          59287\n",
      "CANCELLED                   33170\n",
      "FUSE - PRIMARY              30081\n",
      "CUSTOMER EQUIPMENT          22818\n",
      "ON UPON ARRIVAL             17059\n",
      "CONDUCTOR SECONDARY - OH    14304\n",
      "CONDUCTOR PRIMARY - OH      11989\n",
      "SUBSTATION CIRCUIT          11790\n",
      "OTHER                       11364\n",
      "POLE                        11071\n",
      "TRANSFORMER OH               9213\n",
      "CONDUCTOR SERVICE - OH       8038\n",
      "CONDUCTOR PRIMARY - UG       6496\n",
      "CONNECTOR                    6482\n",
      "JUMPER                       5907\n",
      "CUTOUT                       4226\n",
      "CONDUCTOR SECONDARY - UG     4187\n",
      "FUSE BARREL                  4032\n",
      "CONDUCTOR SERVICE - UG       2524\n",
      "TRANSFORMER UG               2423\n",
      "METER - SECONDARY            1935\n",
      "CROSSARM                     1782\n",
      "LINE RECLOSER                1408\n",
      "ARRESTOR OH                  1392\n",
      "INSULATOR                    1282\n",
      "SCADA - CIRCUIT TRIP         1188\n",
      "SCADA - LOCKOUT              1111\n",
      "NaN                           959\n",
      "PEDESTAL/J-BOX                910\n",
      "PIN                           525\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Raw Category Distribution (before merges):\n",
      "EDA_Equipment_Group\n",
      "Fuse                  98392\n",
      "Customer_Equipment    93796\n",
      "Conductor             47938\n",
      "Infrastructure        26090\n",
      "Protection_Device     14704\n",
      "Transformer           11683\n",
      "Power_Management        226\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Category Distribution (after merges):\n",
      "EDA_Equipment_Group_Merged\n",
      "Fuse                  98392\n",
      "Customer_Equipment    93796\n",
      "Conductor             47938\n",
      "Infrastructure        37999\n",
      "Protection_Device     14704\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of categories per substation (top 10 substations):\n",
      "Job Substation      EDA_Equipment_Group_Merged\n",
      "3109:HONOR HEIGHTS  Fuse                          0.436090\n",
      "                    Customer_Equipment            0.224311\n",
      "                    Infrastructure                0.172932\n",
      "                    Conductor                     0.121554\n",
      "                    Protection_Device             0.045113\n",
      "3110:RIVERSIDE      Fuse                          0.379447\n",
      "                    Customer_Equipment            0.205534\n",
      "                    Conductor                     0.183399\n",
      "                    Infrastructure                0.125692\n",
      "                    Protection_Device             0.105929\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample of majority categories per substation:\n",
      "       Job Substation   majority_category  top_share\n",
      "0  3109:HONOR HEIGHTS                Fuse   0.436090\n",
      "1      3110:RIVERSIDE                Fuse   0.379447\n",
      "2    3111:FIVE TRIBES                Fuse   0.395192\n",
      "3      3114: TENNYSON           Conductor   1.000000\n",
      "4       3114:TENNYSON                Fuse   0.395367\n",
      "5        3116:CALLERY                Fuse   0.516234\n",
      "6   3117:THREE RIVERS  Customer_Equipment   0.321513\n",
      "7        3128:HANCOCK                Fuse   0.395150\n",
      "8    3131:MUSKOGEE WW  Customer_Equipment   1.000000\n",
      "9  3132:MUSKOGEE PORT                Fuse   0.356164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npriority_classes = ['Protection_Device','Infrastructure','Conductor']\\nthreshold = 0.2\\n\\ndef priority_label(dist_series):\\n    # dist_series is the value_counts(normalize=True) for a substation\\n    for cls in priority_classes:\\n        if dist_series.get(cls, 0) > threshold:\\n            return cls\\n    return dist_series.idxmax()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell X: Data Exploration and Validation\n",
    "\n",
    "# 1. Inspect the raw \"Equip Desc\" values\n",
    "unique_equip_desc = incident_df['Equip Desc'].unique()\n",
    "print(f\"Number of unique 'Equip Desc' entries: {len(unique_equip_desc)}\")\n",
    "print(\"Sample 'Equip Desc' values:\")\n",
    "print(unique_equip_desc[:20])  # print first 20 for brevity\n",
    "\n",
    "# 2. Frequency distribution of 'Equip Desc'\n",
    "equip_desc_counts = incident_df['Equip Desc'].value_counts(dropna=False)\n",
    "print(\"\\nEquipment Description Counts:\")\n",
    "print(equip_desc_counts.head(30))  # top 30 for brevity\n",
    "\n",
    "# 3. Verify the categorize_equip function\n",
    "#    (assuming you've defined categorize_equip already)\n",
    "def categorize_equip(desc):\n",
    "    desc = str(desc).upper()\n",
    "    EQUIP_GROUP_MAPPING = {\n",
    "        r'\\bFUSE\\b': 'Fuse',\n",
    "        r'\\bCUTOUT\\b': 'Fuse',\n",
    "        r'\\bCONDUCTOR\\b': 'Conductor',\n",
    "        r'\\b(RECLOSER|ARRESTOR|SUBSTATION CIRCUIT|RELAY|CIRCUIT BREAKER|GROUNDING)\\b': 'Protection_Device',\n",
    "        r'\\b(TRANSFORMER|XFMR)\\b': 'Transformer',\n",
    "        r'\\b(CONNECTOR|JUMPER|SPLICE)\\b': 'Infrastructure',\n",
    "        r'\\bCUSTOMER\\b': 'Customer_Equipment',\n",
    "        r'\\b(POLE|CROSSARM|PIN|TOWER|ANCHOR)\\b': 'Infrastructure',\n",
    "        r'\\b(REGULATOR|CAPACITOR)\\b': 'Power_Management'\n",
    "    }\n",
    "    for pattern, group in EQUIP_GROUP_MAPPING.items():\n",
    "        if re.search(pattern, desc, flags=re.IGNORECASE):\n",
    "            return group\n",
    "    return 'Customer_Equipment'\n",
    "\n",
    "incident_df['EDA_Equipment_Group'] = incident_df['Equip Desc'].apply(categorize_equip)\n",
    "\n",
    "# 4. Distribution of categories BEFORE any merges (Transformer -> Infrastructure, etc.)\n",
    "raw_group_counts = incident_df['EDA_Equipment_Group'].value_counts(dropna=False)\n",
    "print(\"\\nRaw Category Distribution (before merges):\")\n",
    "print(raw_group_counts)\n",
    "\n",
    "# 5. Check final category distribution AFTER merging 'Power_Management'/'Transformer' => 'Infrastructure'\n",
    "#    (mirroring your logic)\n",
    "def merge_categories(category):\n",
    "    if category == 'Power_Management':\n",
    "        return 'Infrastructure'\n",
    "    elif category == 'Transformer':\n",
    "        return 'Infrastructure'\n",
    "    else:\n",
    "        return category\n",
    "\n",
    "incident_df['EDA_Equipment_Group_Merged'] = incident_df['EDA_Equipment_Group'].apply(merge_categories)\n",
    "merged_group_counts = incident_df['EDA_Equipment_Group_Merged'].value_counts(dropna=False)\n",
    "print(\"\\nFinal Category Distribution (after merges):\")\n",
    "print(merged_group_counts)\n",
    "\n",
    "# 6. Substation-level analysis\n",
    "#    Let's see how many incidents per substation, and the distribution of final categories.\n",
    "grouped_subs = incident_df.groupby('Job Substation')['EDA_Equipment_Group_Merged'].value_counts(normalize=True)\n",
    "print(\"\\nDistribution of categories per substation (top 10 substations):\")\n",
    "print(grouped_subs.head(10))  # print first 10 substation-group combos\n",
    "\n",
    "# 7. Identify majority categories per substation\n",
    "#    We'll see which category is the top one per substation, ignoring any priority threshold.\n",
    "def majority_category(series):\n",
    "    return series.idxmax()\n",
    "\n",
    "substation_majorities = incident_df.groupby('Job Substation')['EDA_Equipment_Group_Merged'].agg(\n",
    "    majority_category=lambda x: x.value_counts().idxmax(),\n",
    "    top_share=lambda x: x.value_counts(normalize=True).max()\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nSample of majority categories per substation:\")\n",
    "print(substation_majorities.head(10))\n",
    "\n",
    "# 8. Compare with your priority-based logic\n",
    "#    If you have code for priority-based logic, replicate it here to see if there's a mismatch.\n",
    "#    For example (pseudo-code):\n",
    "\"\"\"\n",
    "priority_classes = ['Protection_Device','Infrastructure','Conductor']\n",
    "threshold = 0.2\n",
    "\n",
    "def priority_label(dist_series):\n",
    "    # dist_series is the value_counts(normalize=True) for a substation\n",
    "    for cls in priority_classes:\n",
    "        if dist_series.get(cls, 0) > threshold:\n",
    "            return cls\n",
    "    return dist_series.idxmax()\n",
    "\"\"\"\n",
    "\n",
    "# 9. Summarize how many times each category ends up as the majority vs. your priority-based approach.\n",
    "#    This will reveal if the threshold logic is overshadowing certain categories.\n",
    "#    (Optional, depending on how your code is structured.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Equipment Grouping Recreated!\n",
      "Equipment_Group\n",
      "Fuse                  0.336005\n",
      "Customer_Equipment    0.320310\n",
      "Conductor             0.163706\n",
      "Infrastructure        0.129765\n",
      "Protection_Device     0.050214\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Initial Class Weights (from incident data):\n",
      "Equipment_Group\n",
      "Fuse                  0.074725\n",
      "Conductor             0.153373\n",
      "Infrastructure        0.193489\n",
      "Customer_Equipment    0.078387\n",
      "Protection_Device     0.500026\n",
      "Name: count, dtype: float64\n",
      "Alternative Majority Vote with Infrastructure Override (sample):\n",
      "Job Substation\n",
      "3109:HONOR HEIGHTS        Infrastructure\n",
      "3110:RIVERSIDE                      Fuse\n",
      "3111:FIVE TRIBES                    Fuse\n",
      "3114:TENNYSON                       Fuse\n",
      "3116:CALLERY                        Fuse\n",
      "3117:THREE RIVERS     Customer_Equipment\n",
      "3128:HANCOCK                        Fuse\n",
      "3131:MUSKOGEE WW      Customer_Equipment\n",
      "3132:MUSKOGEE PORT                  Fuse\n",
      "3136:EUCLID                         Fuse\n",
      "Name: EDA_Equipment_Group_Merged, dtype: object\n",
      "\n",
      "New Distribution of Majority Labels (textual):\n",
      "EDA_Equipment_Group_Merged\n",
      "Fuse                  140\n",
      "Customer_Equipment    119\n",
      "Infrastructure         78\n",
      "Protection_Device      20\n",
      "Conductor               5\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'substation_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m substation_new_numeric \u001b[39m=\u001b[39m substation_new_labels\u001b[39m.\u001b[39mmap(class_labels)\n\u001b[1;32m    107\u001b[0m \u001b[39m# Map Labels to Graph Nodes (fill missing nodes with 'Fuse' by default)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m substation_labels \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(substation_labels)\n\u001b[1;32m    109\u001b[0m missing_subs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(hetero_graph[\u001b[39m'\u001b[39m\u001b[39msubstation\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnode_ids) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(substation_labels\u001b[39m.\u001b[39mindex)\n\u001b[1;32m    110\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mNumber of graph nodes missing in incident data: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(missing_subs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'substation_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2: Multi Class Final\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define mapping of equipment descriptions to categories\n",
    "EQUIP_GROUP_MAPPING = {\n",
    "    r'\\bFUSE\\b': 'Fuse',\n",
    "    r'\\bCUTOUT\\b': 'Fuse',\n",
    "    r'\\bCONDUCTOR\\b': 'Conductor',\n",
    "    r'\\b(RECLOSER|ARRESTOR|SUBSTATION CIRCUIT|RELAY|CIRCUIT BREAKER|GROUNDING)\\b': 'Protection_Device',\n",
    "    r'\\b(TRANSFORMER|XFMR)\\b': 'Transformer',\n",
    "    r'\\b(CONNECTOR|JUMPER|SPLICE)\\b': 'Infrastructure',\n",
    "    r'\\bCUSTOMER\\b': 'Customer_Equipment',\n",
    "    r'\\b(POLE|CROSSARM|PIN|TOWER|ANCHOR)\\b': 'Infrastructure',\n",
    "    r'\\b(REGULATOR|CAPACITOR)\\b': 'Power_Management'\n",
    "}\n",
    "\n",
    "def categorize_equip(desc):\n",
    "    \"\"\"Categorize equipment based on description using regex matching.\"\"\"\n",
    "    desc = str(desc).upper()\n",
    "    for pattern, group in EQUIP_GROUP_MAPPING.items():\n",
    "        if re.search(pattern, desc, flags=re.IGNORECASE):\n",
    "            return group\n",
    "    return 'Customer_Equipment'  # Default category\n",
    "\n",
    "# Apply categorization to dataset\n",
    "incident_df['Equipment_Group'] = incident_df['Equip Desc'].apply(categorize_equip)\n",
    "\n",
    "# Replace categories as per earlier merging logic\n",
    "incident_df['Equipment_Group'] = incident_df['Equipment_Group'].replace({\n",
    "    'Power_Management': 'Infrastructure',\n",
    "    'Transformer': 'Infrastructure'\n",
    "})\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nEquipment Grouping Recreated!\")\n",
    "print(incident_df['Equipment_Group'].value_counts(normalize=True))\n",
    "\n",
    "# Clean & Standardize Job Substation names (ensuring consistency with graph)\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "def clean_and_standardize_substations(substation_name):\n",
    "    \"\"\"Cleans and standardizes substation names to '<feeder_id>:<substation_name>' format.\"\"\"\n",
    "    match = re.match(r\"(\\d+)\\s*:\\s*(.+)\", substation_name.strip().upper())\n",
    "    if match:\n",
    "        feeder_id, name = match.groups()\n",
    "        return f\"{feeder_id}:{name.strip()}\"\n",
    "    return None  # Mark invalid format clearly\n",
    "\n",
    "# Apply cleaning function\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].apply(clean_and_standardize_substations)\n",
    "valid_subs = incident_df['Job Substation'].dropna().unique()\n",
    "\n",
    "# Class labels with weighting\n",
    "class_labels = {\n",
    "    'Fuse': 0, \n",
    "    'Conductor': 1, \n",
    "    'Infrastructure': 2,\n",
    "    'Customer_Equipment': 3, \n",
    "    'Protection_Device': 4\n",
    "}\n",
    "\n",
    "# Class Weights with Smoothing (avoid division by zero)\n",
    "class_counts = incident_df['Equipment_Group'].value_counts().reindex(class_labels.keys(), fill_value=1)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "print(\"\\nInitial Class Weights (from incident data):\")\n",
    "print(class_weights)\n",
    "\n",
    "# Set a threshold for Infrastructure proportion.\n",
    "infrastructure_threshold = 0.15\n",
    "\n",
    "def assign_label_with_infra_override(group_series, infra_thresh=infrastructure_threshold):\n",
    "    \"\"\"\n",
    "    Given a series of equipment categories for a substation (from EDA_Equipment_Group_Merged),\n",
    "    compute the normalized frequency (proportions). If 'Infrastructure' proportion meets or exceeds\n",
    "    the threshold, assign 'Infrastructure' as the label; otherwise, assign the most frequent category.\n",
    "    \"\"\"\n",
    "    counts = group_series.value_counts(normalize=True)\n",
    "    # Check if Infrastructure meets the threshold.\n",
    "    if counts.get('Infrastructure', 0) >= infra_thresh:\n",
    "        return 'Infrastructure'\n",
    "    else:\n",
    "        return counts.idxmax()\n",
    "\n",
    "# Apply the new label assignment for each substation.\n",
    "substation_new_labels = incident_df.groupby('Job Substation')['EDA_Equipment_Group_Merged'] \\\n",
    "                                   .agg(assign_label_with_infra_override)\n",
    "\n",
    "print(\"Alternative Majority Vote with Infrastructure Override (sample):\")\n",
    "print(substation_new_labels.head(10))\n",
    "\n",
    "# Check the distribution of the new textual labels.\n",
    "new_label_distribution = substation_new_labels.value_counts()\n",
    "print(\"\\nNew Distribution of Majority Labels (textual):\")\n",
    "print(new_label_distribution)\n",
    "\n",
    "# Map the textual labels to numeric labels using class_labels.\n",
    "substation_new_numeric = substation_new_labels.map(class_labels)\n",
    "\n",
    "# Map Labels to Graph Nodes (fill missing nodes with 'Fuse' by default)\n",
    "substation_labels = pd.Series(substation_labels)\n",
    "missing_subs = set(hetero_graph['substation'].node_ids) - set(substation_labels.index)\n",
    "print(f\"\\nNumber of graph nodes missing in incident data: {len(missing_subs)}\")\n",
    "substation_labels = substation_labels.reindex(hetero_graph['substation'].node_ids, fill_value=class_labels['Fuse'])\n",
    "\n",
    "# Convert to Tensor and assign to graph\n",
    "hetero_graph['substation'].y = torch.tensor(\n",
    "    substation_labels.values.astype(np.int64),\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Process and update class weights:\n",
    "# Clip and normalize weights to avoid extreme values.\n",
    "cw_values = class_weights.values  # convert to numpy array\n",
    "max_weight = np.percentile(cw_values, 90)  # cap extreme weights\n",
    "cw_values = np.clip(cw_values, 0.1, max_weight)\n",
    "cw_values = cw_values / cw_values.sum()  # re-normalize\n",
    "\n",
    "# Assign class weights to the graph\n",
    "hetero_graph['substation'].class_weights = torch.tensor(cw_values, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"\\nFinal Class Weights assigned to graph nodes: {hetero_graph['substation'].class_weights.cpu().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Equipment Grouping Recreated!\n",
      "Equipment_Group\n",
      "Fuse                  0.336005\n",
      "Customer_Equipment    0.320310\n",
      "Conductor             0.163706\n",
      "Infrastructure        0.129765\n",
      "Protection_Device     0.050214\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Initial Class Weights (from incident data):\n",
      "Equipment_Group\n",
      "Fuse                  0.074725\n",
      "Conductor             0.153373\n",
      "Infrastructure        0.193489\n",
      "Customer_Equipment    0.078387\n",
      "Protection_Device     0.500026\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Alternative Majority Vote with Infrastructure Override (sample):\n",
      "Job Substation\n",
      "3109:HONOR HEIGHTS        Infrastructure\n",
      "3110:RIVERSIDE                      Fuse\n",
      "3111:FIVE TRIBES                    Fuse\n",
      "3114:TENNYSON                       Fuse\n",
      "3116:CALLERY                        Fuse\n",
      "3117:THREE RIVERS     Customer_Equipment\n",
      "3128:HANCOCK                        Fuse\n",
      "3131:MUSKOGEE WW      Customer_Equipment\n",
      "3132:MUSKOGEE PORT                  Fuse\n",
      "3136:EUCLID                         Fuse\n",
      "Name: Equipment_Group, dtype: object\n",
      "\n",
      "New Distribution of Majority Labels (textual):\n",
      "Equipment_Group\n",
      "Fuse                  140\n",
      "Customer_Equipment    119\n",
      "Infrastructure         78\n",
      "Protection_Device      20\n",
      "Conductor               5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Class Weights assigned to graph nodes: [0.10819317 0.16593884 0.20934172 0.10819317 0.4083331 ]\n",
      "\n",
      "Final Label Distribution (Numeric):\n",
      "Equipment_Group\n",
      "0    136\n",
      "1      4\n",
      "2     77\n",
      "3    112\n",
      "4     18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Multi Class Final (Optimal Labeling Based on EDA )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 1. Define mapping of equipment descriptions to categories.\n",
    "EQUIP_GROUP_MAPPING = {\n",
    "    r'\\bFUSE\\b': 'Fuse',\n",
    "    r'\\bCUTOUT\\b': 'Fuse',\n",
    "    r'\\bCONDUCTOR\\b': 'Conductor',\n",
    "    r'\\b(RECLOSER|ARRESTOR|SUBSTATION CIRCUIT|RELAY|CIRCUIT BREAKER|GROUNDING)\\b': 'Protection_Device',\n",
    "    r'\\b(TRANSFORMER|XFMR)\\b': 'Transformer',\n",
    "    r'\\b(CONNECTOR|JUMPER|SPLICE)\\b': 'Infrastructure',\n",
    "    r'\\bCUSTOMER\\b': 'Customer_Equipment',\n",
    "    r'\\b(POLE|CROSSARM|PIN|TOWER|ANCHOR)\\b': 'Infrastructure',\n",
    "    r'\\b(REGULATOR|CAPACITOR)\\b': 'Power_Management'\n",
    "}\n",
    "\n",
    "def categorize_equip(desc):\n",
    "    \"\"\"Categorize equipment based on description using regex matching.\"\"\"\n",
    "    desc = str(desc).upper()\n",
    "    for pattern, group in EQUIP_GROUP_MAPPING.items():\n",
    "        if re.search(pattern, desc, flags=re.IGNORECASE):\n",
    "            return group\n",
    "    return 'Customer_Equipment'\n",
    "\n",
    "# 2. Apply categorization and merge categories.\n",
    "incident_df['Equipment_Group'] = incident_df['Equip Desc'].apply(categorize_equip)\n",
    "incident_df['Equipment_Group'] = incident_df['Equipment_Group'].replace({\n",
    "    'Power_Management': 'Infrastructure',\n",
    "    'Transformer': 'Infrastructure'\n",
    "})\n",
    "\n",
    "print(\"\\nEquipment Grouping Recreated!\")\n",
    "print(incident_df['Equipment_Group'].value_counts(normalize=True))\n",
    "\n",
    "# 3. Clean & Standardize Job Substation names.\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "def clean_and_standardize_substations(substation_name):\n",
    "    \"\"\"Cleans and standardizes substation names to '<feeder_id>:<substation_name>' format.\"\"\"\n",
    "    match = re.match(r\"(\\d+)\\s*:\\s*(.+)\", substation_name.strip().upper())\n",
    "    if match:\n",
    "        feeder_id, name = match.groups()\n",
    "        return f\"{feeder_id}:{name.strip()}\"\n",
    "    return None\n",
    "\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].apply(clean_and_standardize_substations)\n",
    "valid_subs = incident_df['Job Substation'].dropna().unique()\n",
    "\n",
    "# 4. Define class labels.\n",
    "class_labels = {\n",
    "    'Fuse': 0, \n",
    "    'Conductor': 1, \n",
    "    'Infrastructure': 2,\n",
    "    'Customer_Equipment': 3, \n",
    "    'Protection_Device': 4\n",
    "}\n",
    "\n",
    "# 5. Compute class weights (with smoothing).\n",
    "class_counts = incident_df['Equipment_Group'].value_counts().reindex(class_labels.keys(), fill_value=1)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "print(\"\\nInitial Class Weights (from incident data):\")\n",
    "print(class_weights)\n",
    "\n",
    "# 6. Set a threshold for the Infrastructure override.\n",
    "infrastructure_threshold = 0.15\n",
    "\n",
    "def assign_label_with_infra_override(group_series, infra_thresh=infrastructure_threshold):\n",
    "    \"\"\"\n",
    "    For a given series of equipment categories for a substation,\n",
    "    if the proportion of 'Infrastructure' is >= infra_thresh, return 'Infrastructure';\n",
    "    otherwise, return the most frequent category.\n",
    "    \"\"\"\n",
    "    counts = group_series.value_counts(normalize=True)\n",
    "    if counts.get('Infrastructure', 0) >= infra_thresh:\n",
    "        return 'Infrastructure'\n",
    "    else:\n",
    "        return counts.idxmax()\n",
    "\n",
    "# 7. Assign substation labels using alternative majority vote with Infrastructure override.\n",
    "substation_new_labels = incident_df.groupby('Job Substation')['Equipment_Group'] \\\n",
    "                                   .agg(assign_label_with_infra_override)\n",
    "\n",
    "print(\"\\nAlternative Majority Vote with Infrastructure Override (sample):\")\n",
    "print(substation_new_labels.head(10))\n",
    "\n",
    "print(\"\\nNew Distribution of Majority Labels (textual):\")\n",
    "print(substation_new_labels.value_counts())\n",
    "\n",
    "# 8. Map textual labels to numeric labels.\n",
    "substation_new_numeric = substation_new_labels.map(class_labels)\n",
    "\n",
    "# 9. Ensure every node in the graph has a label (fill missing with 'Fuse').\n",
    "substation_new_numeric = substation_new_numeric.reindex(hetero_graph['substation'].node_ids, fill_value=class_labels['Fuse'])\n",
    "\n",
    "# 10. Update the graph with these labels.\n",
    "hetero_graph['substation'].y = torch.tensor(substation_new_numeric.values.astype(np.int64),\n",
    "                                             dtype=torch.long,\n",
    "                                             device=device)\n",
    "\n",
    "# 11. Process and update class weights: clip and re-normalize.\n",
    "cw_values = class_weights.values  # convert to numpy array\n",
    "max_weight = np.percentile(cw_values, 90)  # cap extreme weights\n",
    "cw_values = np.clip(cw_values, 0.1, max_weight)\n",
    "cw_values = cw_values / cw_values.sum()  # re-normalize\n",
    "\n",
    "hetero_graph['substation'].class_weights = torch.tensor(cw_values, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"\\nFinal Class Weights assigned to graph nodes: {hetero_graph['substation'].class_weights.cpu().numpy()}\")\n",
    "\n",
    "print(\"\\nFinal Label Distribution (Numeric):\")\n",
    "final_distribution = pd.Series(substation_new_numeric).value_counts().sort_index()\n",
    "print(final_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Equipment Grouping Recreated!\n",
      "Equipment_Group\n",
      "Fuse                  0.336005\n",
      "Customer_Equipment    0.320310\n",
      "Conductor             0.163706\n",
      "Infrastructure        0.129765\n",
      "Protection_Device     0.050214\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of graph nodes missing in incident data: 0\n",
      "\n",
      "Final Label Distribution (Numeric):\n",
      "0    202\n",
      "1      4\n",
      "3    123\n",
      "4     18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell Test: Multi Class Final (No Class Weights)\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define mapping of equipment descriptions to categories\n",
    "EQUIP_GROUP_MAPPING = {\n",
    "    r'\\bFUSE\\b': 'Fuse',\n",
    "    r'\\bCUTOUT\\b': 'Fuse',\n",
    "    r'\\bCONDUCTOR\\b': 'Conductor',\n",
    "    r'\\b(RECLOSER|ARRESTOR|SUBSTATION CIRCUIT|RELAY|CIRCUIT BREAKER|GROUNDING)\\b': 'Protection_Device',\n",
    "    r'\\b(TRANSFORMER|XFMR)\\b': 'Transformer',\n",
    "    r'\\b(CONNECTOR|JUMPER|SPLICE)\\b': 'Infrastructure',\n",
    "    r'\\bCUSTOMER\\b': 'Customer_Equipment',\n",
    "    r'\\b(POLE|CROSSARM|PIN|TOWER|ANCHOR)\\b': 'Infrastructure',\n",
    "    r'\\b(REGULATOR|CAPACITOR)\\b': 'Power_Management'\n",
    "}\n",
    "\n",
    "def categorize_equip(desc):\n",
    "    \"\"\"Categorize equipment based on description using regex matching.\"\"\"\n",
    "    desc = str(desc).upper()\n",
    "    for pattern, group in EQUIP_GROUP_MAPPING.items():\n",
    "        if re.search(pattern, desc, flags=re.IGNORECASE):\n",
    "            return group\n",
    "    return 'Customer_Equipment'  # Default category\n",
    "\n",
    "# Apply categorization to dataset\n",
    "incident_df['Equipment_Group'] = incident_df['Equip Desc'].apply(categorize_equip)\n",
    "\n",
    "# Merge categories (Transformer, Power_Management => Infrastructure)\n",
    "incident_df['Equipment_Group'] = incident_df['Equipment_Group'].replace({\n",
    "    'Power_Management': 'Infrastructure',\n",
    "    'Transformer': 'Infrastructure'\n",
    "})\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nEquipment Grouping Recreated!\")\n",
    "print(incident_df['Equipment_Group'].value_counts(normalize=True))\n",
    "\n",
    "# Clean & Standardize Job Substation names (ensuring consistency with graph)\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "def clean_and_standardize_substations(substation_name):\n",
    "    \"\"\"Cleans and standardizes substation names to '<feeder_id>:<substation_name>' format.\"\"\"\n",
    "    match = re.match(r\"(\\d+)\\s*:\\s*(.+)\", substation_name.strip().upper())\n",
    "    if match:\n",
    "        feeder_id, name = match.groups()\n",
    "        return f\"{feeder_id}:{name.strip()}\"\n",
    "    return None  # Mark invalid format clearly\n",
    "\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].apply(clean_and_standardize_substations)\n",
    "valid_subs = incident_df['Job Substation'].dropna().unique()\n",
    "\n",
    "# Define the class labels (without weighting)\n",
    "class_labels = {\n",
    "    'Fuse': 0, \n",
    "    'Conductor': 1, \n",
    "    'Infrastructure': 2,\n",
    "    'Customer_Equipment': 3, \n",
    "    'Protection_Device': 4\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# REMOVE CLASS WEIGHT CALCULATION ENTIRELY\n",
    "# (No class_weights assigned to the graph)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We'll do a simple majority vote approach. If you already have\n",
    "# an alternative method, replace the logic below with it.\n",
    "\n",
    "substation_labels = {}\n",
    "for substation, group in incident_df.groupby('Job Substation'):\n",
    "    # If substation has no incidents, default to Fuse\n",
    "    if len(group) == 0:\n",
    "        substation_labels[substation] = class_labels['Fuse']\n",
    "        continue\n",
    "    \n",
    "    # Simple majority vote\n",
    "    class_dist = group['Equipment_Group'].value_counts()\n",
    "    substation_labels[substation] = class_labels[class_dist.idxmax()]\n",
    "\n",
    "substation_labels = pd.Series(substation_labels)\n",
    "missing_subs = set(hetero_graph['substation'].node_ids) - set(substation_labels.index)\n",
    "print(f\"\\nNumber of graph nodes missing in incident data: {len(missing_subs)}\")\n",
    "\n",
    "# Fill missing nodes with Fuse by default\n",
    "substation_labels = substation_labels.reindex(hetero_graph['substation'].node_ids, fill_value=class_labels['Fuse'])\n",
    "\n",
    "# Convert to Tensor and assign to graph\n",
    "hetero_graph['substation'].y = torch.tensor(\n",
    "    substation_labels.values.astype(np.int64),\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print final label distribution\n",
    "final_distribution = pd.Series(substation_labels).value_counts().sort_index()\n",
    "print(\"\\nFinal Label Distribution (Numeric):\")\n",
    "print(final_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Stratified Splitting of the Dataset (Enhanced with Fallback)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Assume final_labels contains the final numeric substation labels from Cell 2.\n",
    "# We'll use substation_new_numeric as our final labels.\n",
    "final_labels = substation_new_numeric.copy()  # Series indexed by substation names, values: numeric labels\n",
    "\n",
    "print(\"Overall Label Distribution for Stratified Split:\")\n",
    "print(final_labels.value_counts())\n",
    "\n",
    "# Convert index and labels to numpy arrays.\n",
    "substations = np.array(final_labels.index)\n",
    "labels = final_labels.values\n",
    "\n",
    "# First, split into train (70%) and temporary (30%)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
    "for train_index, temp_index in sss.split(substations, labels):\n",
    "    train_substations = substations[train_index]\n",
    "    temp_substations = substations[temp_index]\n",
    "\n",
    "# Check the distribution in the temporary set.\n",
    "temp_labels = final_labels.loc[temp_substations].values\n",
    "unique, counts = np.unique(temp_labels, return_counts=True)\n",
    "print(\"Temporary set label counts:\", dict(zip(unique, counts)))\n",
    "\n",
    "# If any class in the temporary set has fewer than 2 samples, use a random split.\n",
    "if np.min(np.bincount(temp_labels)) < 2:\n",
    "    print(\"Warning: Some classes in the temporary set have fewer than 2 members. Using random split for validation/test.\")\n",
    "    indices = np.arange(len(temp_substations))\n",
    "    np.random.shuffle(indices)\n",
    "    split_idx = int(len(temp_substations) * 0.5)\n",
    "    val_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    val_substations = temp_substations[val_indices]\n",
    "    test_substations = temp_substations[test_indices]\n",
    "else:\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    for val_index, test_index in sss2.split(temp_substations, temp_labels):\n",
    "        val_substations = temp_substations[val_index]\n",
    "        test_substations = temp_substations[test_index]\n",
    "\n",
    "# Display counts in each split.\n",
    "print(\"\\nSubstation counts in each split:\")\n",
    "print(f\"Train: {len(train_substations)}, Validation: {len(val_substations)}, Test: {len(test_substations)}\")\n",
    "\n",
    "# Now, create boolean masks for each node in the heterogeneous graph.\n",
    "node_ids = hetero_graph['substation'].node_ids  # List of substation names from the graph.\n",
    "train_mask = torch.tensor([sub in set(train_substations) for sub in node_ids], dtype=torch.bool, device=device)\n",
    "val_mask = torch.tensor([sub in set(val_substations) for sub in node_ids], dtype=torch.bool, device=device)\n",
    "test_mask = torch.tensor([sub in set(test_substations) for sub in node_ids], dtype=torch.bool, device=device)\n",
    "\n",
    "# Assign these masks to the graph.\n",
    "hetero_graph['substation'].train_mask = train_mask\n",
    "hetero_graph['substation'].val_mask = val_mask\n",
    "hetero_graph['substation'].test_mask = test_mask\n",
    "\n",
    "# Print final counts and overall label distribution.\n",
    "print(\"\\nFinal Train/Val/Test Split:\")\n",
    "print(f\"Train: {train_mask.sum().item()} nodes, Validation: {val_mask.sum().item()} nodes, Test: {test_mask.sum().item()} nodes\")\n",
    "print(\"\\nOverall Final Label Distribution:\")\n",
    "print(pd.Series(final_labels).value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final class distribution: {0: 202, 3: 123, 4: 18, 1: 4}\n",
      "Edge normalization complete\n",
      "Final Train/Val/Test split: 251/52/44\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Splitting the dataset \n",
    "\n",
    "# Get unique substations sorted by earliest incident\n",
    "substation_earliest_time = incident_df.groupby('Job Substation')['Job OFF Time'].min()\n",
    "\n",
    "sorted_substations = substation_earliest_time.sort_values().index\n",
    "\n",
    "# Define split percentages\n",
    "train_ratio = 0.70  \n",
    "val_ratio = 0.15 \n",
    "test_ratio = 0.15 \n",
    "\n",
    "# Compute split indices\n",
    "train_cutoff = int(len(sorted_substations) * train_ratio)\n",
    "val_cutoff = int(len(sorted_substations) * (train_ratio + val_ratio))\n",
    "\n",
    "# Assign substations to splits\n",
    "train_substations = set(sorted_substations[:train_cutoff])\n",
    "val_substations = set(sorted_substations[train_cutoff:val_cutoff])\n",
    "test_substations = set(sorted_substations[val_cutoff:])\n",
    "\n",
    "# Initialize masks as False\n",
    "train_mask = torch.zeros(len(hetero_graph['substation'].node_ids), dtype=torch.bool, device=device)\n",
    "val_mask = torch.zeros(len(hetero_graph['substation'].node_ids), dtype=torch.bool, device=device)\n",
    "test_mask = torch.zeros(len(hetero_graph['substation'].node_ids), dtype=torch.bool, device=device)\n",
    "\n",
    "# Assign substations to the correct mask\n",
    "for i, sub in enumerate(hetero_graph['substation'].node_ids):\n",
    "    if sub in train_substations:\n",
    "        train_mask[i] = True\n",
    "    elif sub in val_substations:\n",
    "        val_mask[i] = True\n",
    "    elif sub in test_substations:\n",
    "        test_mask[i] = True\n",
    "\n",
    "# Assign to the graph\n",
    "hetero_graph['substation'].train_mask = train_mask\n",
    "hetero_graph['substation'].val_mask = val_mask\n",
    "hetero_graph['substation'].test_mask = test_mask\n",
    "\n",
    "# Step 6: Print final counts\n",
    "assert hetero_graph['substation'].y.dtype == torch.long, \"Labels must be long dtype\"\n",
    "assert hetero_graph['substation'].class_weights.shape[0] == len(class_labels), \"Class weight mismatch\"\n",
    "print(f\"\\nFinal class distribution: {pd.Series(substation_labels).value_counts().to_dict()}\")\n",
    "print(\"Edge normalization complete\")\n",
    "print(f\"Final Train/Val/Test split: {train_mask.sum().item()}/{val_mask.sum().item()}/{test_mask.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Definition and Training for Multiclass Classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# -------------------------\n",
    "# Weighted GCN Convolution\n",
    "# -------------------------\n",
    "class WeightedGCNConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, edge_attr_dim):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, out_channels, add_self_loops=False)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_attr_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm1d(out_channels)\n",
    "        self.residual_proj = nn.Linear(in_channels, out_channels) if in_channels != out_channels else nn.Identity()\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for layer in self.edge_mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(layer.bias, 0.1)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        if edge_index.size(1) == 0:\n",
    "            return self.residual_proj(x)  # Handle empty edge case\n",
    "        # Adjust edge MLP if needed\n",
    "        edge_dim = edge_attr.shape[1] if edge_attr.dim() > 1 else 1\n",
    "        if edge_dim != self.edge_mlp[0].in_features:\n",
    "            print(f\"⚠ Adjusting edge MLP: Expected {self.edge_mlp[0].in_features}, got {edge_dim}\")\n",
    "            self.edge_mlp[0] = nn.Linear(edge_dim, 64).to(edge_attr.device)\n",
    "        # Normalize edge attributes\n",
    "        edge_attr = (edge_attr - edge_attr.mean(dim=0)) / (edge_attr.std(dim=0) + 1e-8)\n",
    "        edge_attr = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        weights = self.edge_mlp(edge_attr).squeeze()\n",
    "        out = self.gcn(x, edge_index, edge_weight=weights + 1e-8)\n",
    "        out = self.batch_norm(out)\n",
    "        return out + self.residual_proj(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Multiclass Heterogeneous GNN Model\n",
    "# ---------------------------------------\n",
    "class PowerGridGNN_Multiclass(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, edge_dims, num_classes, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.edge_types = list(edge_dims.keys())\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_dim = in_channels\n",
    "        \n",
    "        # Build GCN layers for each edge type\n",
    "        for _ in range(num_layers):\n",
    "            layer_dict = nn.ModuleDict({\n",
    "                et: WeightedGCNConv(current_dim, hidden_dim, edge_dims[et])\n",
    "                for et in self.edge_types\n",
    "            })\n",
    "            self.layers.append(layer_dict)\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Multiclass prediction head (raw logits; CrossEntropyLoss applies softmax)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Learnable aggregation weights for combining messages from different edge types\n",
    "        self.aggregation_weights = nn.Parameter(torch.ones(len(self.edge_types)))\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "    \n",
    "    def _register_edge_stats(self, data):\n",
    "        \"\"\"Compute and store edge attribute statistics for normalization.\"\"\"\n",
    "        for et in self.edge_types:\n",
    "            attr = data['substation', et, 'substation'].edge_attr\n",
    "            self.register_buffer(f'{et}_mean', attr.mean(dim=0))\n",
    "            self.register_buffer(f'{et}_std', attr.std(dim=0) + 1e-8)\n",
    "    \n",
    "    def _get_edge_masks(self, data, mode='train'):\n",
    "        \"\"\"Generate edge masks so that only edges connecting nodes within the current split are used.\"\"\"\n",
    "        masks = {}\n",
    "        split_mask = getattr(data['substation'], f\"{mode}_mask\").bool().to(data['substation'].x.device)\n",
    "        for et in self.edge_types:\n",
    "            edge_info = data['substation', et, 'substation']\n",
    "            edge_index = edge_info.edge_index\n",
    "            source_mask = split_mask[edge_index[0]]\n",
    "            target_mask = split_mask[edge_index[1]]\n",
    "            masks[et] = source_mask & target_mask\n",
    "        return masks\n",
    "    \n",
    "    def forward(self, data, mode='train'):\n",
    "        x = data['substation'].x\n",
    "        edge_masks = self._get_edge_masks(data, mode)\n",
    "        \n",
    "        # Message passing through each layer\n",
    "        for layer in self.layers:\n",
    "            messages = []\n",
    "            for et in self.edge_types:\n",
    "                edge_data = data['substation', et, 'substation']\n",
    "                idx, attr = edge_data.edge_index, edge_data.edge_attr\n",
    "                mask = edge_masks[et]\n",
    "                idx, attr = idx[:, mask], attr[mask]\n",
    "                if idx.shape[1] == 0:\n",
    "                    messages.append(x.new_zeros(x.size(0), layer[et].gcn.out_channels))\n",
    "                else:\n",
    "                    # Normalize edge attributes using stored statistics\n",
    "                    attr = (attr - getattr(self, f'{et}_mean')) / getattr(self, f'{et}_std')\n",
    "                    messages.append(layer[et](x, idx, attr))\n",
    "            # Aggregate messages across edge types with learnable weights\n",
    "            x = torch.stack(messages, dim=0)  # Shape: (num_edge_types, num_nodes, hidden_dim)\n",
    "            x = torch.sum(x * F.softmax(self.aggregation_weights, dim=0)[:, None, None], dim=0)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "        \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Hyperparameters and Setup\n",
    "# -------------------------\n",
    "in_channels = hetero_graph['substation'].x.size(1)\n",
    "hidden_dim = 128\n",
    "num_classes = 5  # According to class_labels mapping\n",
    "edge_attr_dims = {'spatial': 8, 'temporal': 2, 'causal': 13}\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "\n",
    "model = PowerGridGNN_Multiclass(in_channels, hidden_dim, edge_attr_dims, num_classes, num_layers=2).to(device)\n",
    "model._register_edge_stats(hetero_graph)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(weight=hetero_graph['substation'].class_weights)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.0289 | Val Loss: 2.4637 | Train Acc: 0.2948\n",
      "Epoch 10 | Train Loss: 1.0227 | Val Loss: 1.4798 | Train Acc: 0.5578\n",
      "Epoch 20 | Train Loss: 0.8359 | Val Loss: 1.5541 | Train Acc: 0.6494\n",
      "Early stopping!\n",
      "\n",
      "Test Metrics:\n",
      "Test Accuracy: 0.5455\n",
      "Test Macro F1: 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.144775.bright04/ipykernel_928031/227479986.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_multiclass_model.pt'))\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "hetero_graph = hetero_graph.to(device)  \n",
    "\n",
    "best_val_loss = float('inf')\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits = model(hetero_graph, mode='train')\n",
    "    loss = criterion(logits[hetero_graph['substation'].train_mask],\n",
    "                     hetero_graph['substation'].y[hetero_graph['substation'].train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(hetero_graph, mode='val')\n",
    "        val_loss = criterion(val_logits[hetero_graph['substation'].val_mask],\n",
    "                             hetero_graph['substation'].y[hetero_graph['substation'].val_mask])\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_multiclass_model.pt')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        train_preds = torch.argmax(logits[hetero_graph['substation'].train_mask], dim=1)\n",
    "        train_labels = hetero_graph['substation'].y[hetero_graph['substation'].train_mask]\n",
    "        train_acc = (train_preds == train_labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    if no_improve >= patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# Test Evaluation\n",
    "\n",
    "model.load_state_dict(torch.load('best_multiclass_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits = model(hetero_graph, mode='test')\n",
    "    test_preds = torch.argmax(test_logits[hetero_graph['substation'].test_mask], dim=1)\n",
    "    test_labels = hetero_graph['substation'].y[hetero_graph['substation'].test_mask]\n",
    "    test_acc = (test_preds == test_labels).float().mean().item()\n",
    "    test_f1 = f1_score(test_labels.cpu().numpy(), test_preds.cpu().numpy(), average='macro')\n",
    "    \n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Macro F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Diagnostic Analysis of Test Predictions\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Ensure model is in evaluation mode and load best model\n",
    "model.load_state_dict(torch.load('best_multiclass_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_logits = model(hetero_graph, mode='test')\n",
    "    test_preds = torch.argmax(test_logits[hetero_graph['substation'].test_mask], dim=1)\n",
    "    test_labels = hetero_graph['substation'].y[hetero_graph['substation'].test_mask]\n",
    "\n",
    "# Print prediction distribution and true labels distribution\n",
    "unique_preds, counts_preds = torch.unique(test_preds, return_counts=True)\n",
    "unique_labels, counts_labels = torch.unique(test_labels, return_counts=True)\n",
    "print(\"Test Predictions Distribution:\")\n",
    "for cls, count in zip(unique_preds.cpu().numpy(), counts_preds.cpu().numpy()):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "\n",
    "print(\"\\nTest Ground Truth Distribution:\")\n",
    "for cls, count in zip(unique_labels.cpu().numpy(), counts_labels.cpu().numpy()):\n",
    "    print(f\"Class {cls}: {count}\")\n",
    "\n",
    "# Compute and display confusion matrix\n",
    "cm = confusion_matrix(test_labels.cpu().numpy(), test_preds.cpu().numpy(), labels=list(class_labels.values()))\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[k for k in class_labels.keys()],\n",
    "            yticklabels=[k for k in class_labels.keys()])\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = hetero_graph['substation'].y[hetero_graph['substation'].train_mask].cpu().numpy()\n",
    "val_labels = hetero_graph['substation'].y[hetero_graph['substation'].val_mask].cpu().numpy()\n",
    "test_labels = hetero_graph['substation'].y[hetero_graph['substation'].test_mask].cpu().numpy()\n",
    "\n",
    "print(\"Train Label Distribution:\", pd.Series(train_labels).value_counts())\n",
    "print(\"Val Label Distribution:\", pd.Series(val_labels).value_counts())\n",
    "print(\"Test Label Distribution:\", pd.Series(test_labels).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Random Substation Split\n",
    "\n",
    "import random\n",
    "\n",
    "substations = hetero_graph['substation'].node_ids\n",
    "random.shuffle(substations)  # Shuffle in place\n",
    "\n",
    "num_nodes = len(substations)\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "train_cutoff = int(num_nodes * train_ratio)\n",
    "val_cutoff = int(num_nodes * (train_ratio + val_ratio))\n",
    "\n",
    "train_substations = set(substations[:train_cutoff])\n",
    "val_substations = set(substations[train_cutoff:val_cutoff])\n",
    "test_substations = set(substations[val_cutoff:])\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "\n",
    "sub_to_idx = {sub: idx for idx, sub in enumerate(hetero_graph['substation'].node_ids)}\n",
    "\n",
    "for i, sub in enumerate(hetero_graph['substation'].node_ids):\n",
    "    if sub in train_substations:\n",
    "        train_mask[i] = True\n",
    "    elif sub in val_substations:\n",
    "        val_mask[i] = True\n",
    "    elif sub in test_substations:\n",
    "        test_mask[i] = True\n",
    "\n",
    "hetero_graph['substation'].train_mask = train_mask\n",
    "hetero_graph['substation'].val_mask = val_mask\n",
    "hetero_graph['substation'].test_mask = test_mask\n",
    "\n",
    "print(f\"Train: {train_mask.sum().item()}, Val: {val_mask.sum().item()}, Test: {test_mask.sum().item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
