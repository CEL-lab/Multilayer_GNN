{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# IMPORTS\n",
    "# ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from itertools import combinations\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load the Excel Dataset\n",
    "# ----------------------------\n",
    "file_path = 'Incidents.xlsx'  # update with your file path\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Track missing values BEFORE imputation\n",
    "missing_before = {\n",
    "    \"Job City (Before)\": df['Job City'].isnull().sum(),\n",
    "    \"Device Address (Before)\": df['Device Address'].isnull().sum()\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# 2. CRITICAL FILTERS\n",
    "# ------------------------------\n",
    "\n",
    "# Only drop rows missing 'Job Substation'\n",
    "df = df.dropna(subset=['Job Substation'])\n",
    "\n",
    "# Convert date columns (errors become NaT)\n",
    "df['Job OFF Time'] = pd.to_datetime(df['Job OFF Time'], errors='coerce')\n",
    "df['Job ON Time']  = pd.to_datetime(df['Job ON Time'], errors='coerce')\n",
    "\n",
    "# Keep rows where both times exist and OFF <= ON\n",
    "time_mask = (\n",
    "    df['Job OFF Time'].notna() &\n",
    "    df['Job ON Time'].notna() &\n",
    "    (df['Job OFF Time'] <= df['Job ON Time'])\n",
    ")\n",
    "df = df[time_mask]\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Data Cleaning: Standardize Text Columns\n",
    "# ----------------------------\n",
    "# Standardize Job City and location fields (remove extra spaces, uppercase, etc.)\n",
    "for col in ['Job City', 'Job Area (DISTRICT)', 'Job Substation', 'Job Feeder']:\n",
    "    df[col] = df[col].astype(str).str.upper().str.strip()\n",
    "\n",
    "# IMPORTANT: Replace string \"NAN\" with actual np.nan for Job City\n",
    "#df['Job City'] = df['Job City'].replace('NAN', np.nan)\n",
    "df['Job City'] = df['Job City'].replace(['NAN', '0'], np.nan)\n",
    "# ----------------------------\n",
    "# 4. Create a Composite Key\n",
    "# ----------------------------\n",
    "# We combine Job Area (DISTRICT), Job Substation, and Feeder ID (as a string) for maximum granularity.\n",
    "#df['Feeder_ID_str'] = df['Feeder ID'].astype(str)  # convert numeric Feeder ID to string if needed\n",
    "df['Feeder_ID_str'] = df['Feeder ID'].astype('Int64').astype(str).replace('<NA>', 'UNKNOWN')\n",
    "df['CompositeKey'] = df['Job Area (DISTRICT)'] + \"_\" + df['Job Substation'] + \"_\" + df['Feeder_ID_str']\n",
    "\n",
    "# Optionally, if you believe Job Feeder is more descriptive than Feeder ID, you can use:\n",
    "# df['CompositeKey'] = df['Job Area (DISTRICT)'] + \"_\" + df['Job Substation'] + \"_\" + df['Job Feeder']\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Impute Missing Job City Using the Composite Key\n",
    "# ----------------------------\n",
    "def get_mode(series):\n",
    "    mode_series = series.mode()\n",
    "    return mode_series.iloc[0] if not mode_series.empty else np.nan\n",
    "\n",
    "# Build the mapping: for each CompositeKey, get the most common (mode) Job City among rows where it's not missing.\n",
    "composite_city_map = (df[df['Job City'].notnull()]\n",
    "                      .groupby('CompositeKey')['Job City']\n",
    "                      .agg(get_mode)\n",
    "                      .to_dict())\n",
    "\n",
    "# Impute missing Job City using the composite key mapping\n",
    "def impute_composite_job_city(row):\n",
    "    if pd.isnull(row['Job City']) or row['Job City'] in ['NAN', '0']:  # Treat \"0\" as missing\n",
    "        return composite_city_map.get(row['CompositeKey'], np.nan)\n",
    "    return row['Job City']\n",
    "\n",
    "\n",
    "df['Job City_imputed'] = df.apply(impute_composite_job_city, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. (Optional) Fallback for Job City Using Job Region\n",
    "# ----------------------------\n",
    "remaining_missing = df['Job City_imputed'].isnull().sum()\n",
    "if remaining_missing > 0:\n",
    "    region_to_city = (df[df['Job City_imputed'].notnull()]\n",
    "                      .groupby('Job Region')['Job City_imputed']\n",
    "                      .agg(get_mode)\n",
    "                      .to_dict())\n",
    "    def fallback_impute_job_city(row):\n",
    "        if pd.isnull(row['Job City_imputed']):\n",
    "            return region_to_city.get(row['Job Region'], np.nan)\n",
    "        return row['Job City_imputed']\n",
    "    df['Job City_imputed'] = df.apply(fallback_impute_job_city, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Impute Missing Device Address Using a Similar Composite Approach\n",
    "# ----------------------------\n",
    "# Standardize Device Address\n",
    "df['Device Address'] = df['Device Address'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Replace the string \"NAN\" with actual np.nan\n",
    "#df['Device Address'] = df['Device Address'].replace('NAN', np.nan)\n",
    "df['Device Address'] = df['Device Address'].replace(['NAN', '0'], np.nan)\n",
    "\n",
    "# Build the mapping: For each CompositeKey, get the most common Device Address (ignoring missing values)\n",
    "composite_address_map = (df[df['Device Address'].notnull()]\n",
    "                          .groupby('CompositeKey')['Device Address']\n",
    "                          .agg(get_mode)\n",
    "                          .to_dict())\n",
    "\n",
    "def impute_composite_device_address(row):\n",
    "    if pd.isnull(row['Device Address']) or row['Device Address'] in ['NAN', '0']:  # Treat \"0\" as missing\n",
    "        return composite_address_map.get(row['CompositeKey'], np.nan)\n",
    "    return row['Device Address']\n",
    "    \n",
    "df['Device Address_imputed'] = df.apply(impute_composite_device_address, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 8. (Optional) Drop Temporary Columns \n",
    "# ----------------------------\n",
    "df = df.drop(columns=['Feeder_ID_str', 'CompositeKey'])\n",
    "\n",
    "# ------------------------------\n",
    "# 9. DROP NON-CRITICAL COLUMNS and Save the Result\n",
    "# ------------------------------\n",
    "cols_to_drop = [\n",
    "    'Lead Crew Phone', 'AM Notes', \n",
    "    'Equipment Desc that should be excluded from reported indices',\n",
    "    'Ark Grid Mod or OK Grid Enhancement Circuits'\n",
    "]\n",
    "#df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "df.to_excel(\"Incidents_imputed.xlsx\", index=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 10. FINAL CHECK\n",
    "# ------------------------------\n",
    "\n",
    "# Track missing values AFTER imputation\n",
    "missing_after = {\n",
    "    \"Job City (After)\": df['Job City_imputed'].isnull().sum(),\n",
    "    \"Device Address (After)\": df['Device Address_imputed'].isnull().sum()\n",
    "}\n",
    "\n",
    "# Display the difference (without displaying the entire dataset)\n",
    "print(\"\\n=== Missing Values Before & After Imputation ===\")\n",
    "for key in missing_before:\n",
    "    before = missing_before[key]\n",
    "    after = missing_after[key.replace(\"(Before)\", \"(After)\")]\n",
    "    reduction = before - after\n",
    "    percent_reduction = (reduction / before * 100) if before > 0 else 0\n",
    "    print(f\"{key}: {before} → {after} (Reduced by {reduction} | {percent_reduction:.2f}%)\")\n",
    "\n",
    "# Final check for remaining missing values\n",
    "print(\"\\nFinal dataset shape:\", df.shape)\n",
    "\n",
    "# Display sample data (only the relevant columns)\n",
    "print(\"\\nSample of Job City and Device Address before & after imputation:\")\n",
    "print(df[['Job City', 'Job City_imputed', 'Device Address', 'Device Address_imputed']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV files\n",
    "substation_df = pd.read_csv(\"Substation.csv\")\n",
    "lines_df = pd.read_csv(\"Lines.csv\")\n",
    "\n",
    "# Standardize text columns (remove extra spaces, uppercase)\n",
    "text_cols = ['Name', 'ADDRESSLIN', 'TOWN']\n",
    "for col in text_cols:\n",
    "    if col in substation_df.columns:\n",
    "        substation_df[col] = substation_df[col].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Standardize text fields in Lines.csv (only LINENAME needed)\n",
    "if 'LINENAME' in lines_df.columns:\n",
    "    lines_df['LINENAME'] = lines_df['LINENAME'].astype(str).str.upper().str.strip()\n",
    "\n",
    "print(\"✅ Step 1: Data read and standardized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_voltage(line_name):\n",
    "    \"\"\"Extract voltage from line name (e.g., '345kV Ranch Road').\"\"\"\n",
    "    match = re.search(r'(\\d+)kV', line_name)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Extract voltages from Lines.csv\n",
    "line_voltage_map = {}\n",
    "for _, row in lines_df.iterrows():\n",
    "    line_voltage = extract_voltage(row['LINENAME'])\n",
    "    if not line_voltage:\n",
    "        continue  # Skip lines where voltage couldn't be determined\n",
    "\n",
    "    # Identify substations in the line name\n",
    "    for substation in substation_df['Name']:\n",
    "        if substation in row['LINENAME']:\n",
    "            line_voltage_map[substation] = line_voltage  # Assign voltage\n",
    "\n",
    "# Apply inferred voltages\n",
    "substation_df['Voltage'] = substation_df.apply(\n",
    "    lambda row: line_voltage_map.get(row['Name'], row['Voltage']), axis=1\n",
    ")\n",
    "\n",
    "print(\"✅ Step 2: Missing voltages inferred using transmission line data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load incident data to extract feeder-based voltages\n",
    "incident_df = pd.read_excel(\"Incidents_imputed.xlsx\", engine='openpyxl')\n",
    "\n",
    "def infer_voltage_from_feeders(substation_df, incident_df):\n",
    "    \"\"\"Infer missing substation voltages using feeder-based data from incidents.\"\"\"\n",
    "    \n",
    "    # Build Feeder ID to Substation mapping\n",
    "    feeder_map = incident_df.groupby('Feeder ID')['Job Substation'].apply(set).to_dict()\n",
    "    \n",
    "    # Dictionary to store inferred voltages\n",
    "    feeder_voltages = {}\n",
    "\n",
    "    for feeder, substations in feeder_map.items():\n",
    "        # Get known voltages for substations in this feeder\n",
    "        known_voltages = substation_df[substation_df['Name'].isin(substations)]['Voltage']\n",
    "        valid_voltages = known_voltages[known_voltages > 0]  # Ignore zero voltages\n",
    "        \n",
    "        if not valid_voltages.empty:\n",
    "            # Assign most common voltage for the feeder\n",
    "            most_common_voltage = valid_voltages.mode()[0]  # Mode is the most frequent value\n",
    "            for sub in substations:\n",
    "                feeder_voltages[sub] = most_common_voltage\n",
    "\n",
    "    # Apply inferred voltages\n",
    "    substation_df['Voltage'] = substation_df.apply(\n",
    "        lambda row: feeder_voltages.get(row['Name'], row['Voltage']), axis=1\n",
    "    )\n",
    "\n",
    "    return substation_df\n",
    "\n",
    "# Apply voltage inference\n",
    "substation_df = infer_voltage_from_feeders(substation_df, incident_df)\n",
    "print(\"✅ Step 3: Remaining missing voltages inferred using feeder data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining missing or zero voltages\n",
    "remaining_missing = substation_df[substation_df['Voltage'].isna() | (substation_df['Voltage'] == 0)].shape[0]\n",
    "\n",
    "if remaining_missing > 0:\n",
    "    print(f\"⚠️ {remaining_missing} substations still have missing voltages after inference.\")\n",
    "else:\n",
    "    print(\"✅ Step 4: All substations now have inferred voltages.\")\n",
    "\n",
    "# Save cleaned Substation data\n",
    "substation_df.to_csv(\"Substation_Cleaned.csv\", index=False)\n",
    "print(\"✅ Cleaned Substation data saved as 'Substation_Cleaned.csv'.\")\n",
    "\n",
    "# Save standardized Lines data (No changes except name formatting)\n",
    "lines_df.to_csv(\"Lines_Cleaned.csv\", index=False)\n",
    "print(\"✅ Standardized Lines data saved as 'Lines_Cleaned.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_voltage_from_region(substation_df):\n",
    "    \"\"\"Infer missing voltage using the most common voltage in the same region.\"\"\"\n",
    "    \n",
    "    # Get most common voltage per region\n",
    "    region_voltage_map = (substation_df[substation_df['Voltage'] > 0]\n",
    "                          .groupby('TOWN')['Voltage']\n",
    "                          .agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "                          .to_dict())\n",
    "    \n",
    "    # Get most common voltage overall (fallback)\n",
    "    most_common_voltage = substation_df[substation_df['Voltage'] > 0]['Voltage'].mode()[0]\n",
    "    \n",
    "    # Apply region-based inference\n",
    "    substation_df['Voltage'] = substation_df.apply(\n",
    "        lambda row: region_voltage_map.get(row['TOWN'], most_common_voltage) \n",
    "        if pd.isnull(row['Voltage']) or row['Voltage'] == 0 else row['Voltage'], axis=1\n",
    "    )\n",
    "\n",
    "    return substation_df\n",
    "\n",
    "# Apply the function\n",
    "substation_df = infer_voltage_from_region(substation_df)\n",
    "\n",
    "# Verify again\n",
    "remaining_missing = substation_df[substation_df['Voltage'].isna() | (substation_df['Voltage'] == 0)].shape[0]\n",
    "if remaining_missing > 0:\n",
    "    print(f\"⚠️ {remaining_missing} substations STILL have missing voltages after region-based inference.\")\n",
    "else:\n",
    "    print(\"✅ All substations now have voltage values after final inference.\")\n",
    "\n",
    "# Save the FINAL cleaned file\n",
    "substation_df.to_csv(\"Substation_Cleaned_Final.csv\", index=False)\n",
    "print(\"✅ Final cleaned Substation data saved as 'Substation_Cleaned_Final.csv'.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# # Preprocessing for Device Type Target Variable\n",
    "# \n",
    "# This notebook performs the following steps:\n",
    "# \n",
    "# 1. Loads the incident dataset.\n",
    "# 2. Identifies and imputes missing values in the \"Device Type\" column using related columns (\"Equip Desc\" and \"Dev Subtype\").\n",
    "# 3. Remaps mis-coded \"0\" values based on heuristic rules.\n",
    "# 4. Defines a final mapping that preserves meaningful categories (Load, Fuse, Recloser, Switch, Breaker) and then merges ambiguous ones (Unknown, Source, Other) into Fuse (Option 2).\n",
    "# 5. Maps the final string labels to integer labels.\n",
    "# 6. Saves the cleaned DataFrame to an Excel file for use in your main pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# ======================\n",
    "# 1. Data Loading\n",
    "# ======================\n",
    "incident_df = pd.read_excel('Incidents_imputed.xlsx', engine='openpyxl')\n",
    "print(\"Initial data shape:\", incident_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. Device Type Imputation\n",
    "# ======================\n",
    "# Create mappings from existing data\n",
    "equip_mapping = (\n",
    "    incident_df.dropna(subset=['Device Type'])\n",
    "    .groupby('Equip Desc')['Device Type']\n",
    "    .agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
    ")\n",
    "\n",
    "subtype_mapping = (\n",
    "    incident_df.dropna(subset=['Device Type'])\n",
    "    .groupby('Dev Subtype')['Device Type']\n",
    "    .agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
    ")\n",
    "\n",
    "# Imputation function with priority logic\n",
    "def impute_device_type(row):\n",
    "    if pd.notna(row['Device Type']):\n",
    "        return row['Device Type']\n",
    "    if row['Equip Desc'] in equip_mapping:\n",
    "        return equip_mapping[row['Equip Desc']]\n",
    "    if row['Dev Subtype'] in subtype_mapping:\n",
    "        return subtype_mapping[row['Dev Subtype']]\n",
    "    return np.nan  # Temporary placeholder\n",
    "\n",
    "incident_df['Device Type_imputed'] = incident_df.apply(impute_device_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3. Handle Special Cases (\"0\" values)\n",
    "# ======================\n",
    "def map_zero_device_type(equip_desc):\n",
    "    \"\"\"Improved mapping with regex patterns\"\"\"\n",
    "    desc = str(equip_desc).lower()\n",
    "    \n",
    "    if re.search(r'\\bfuse\\b', desc):\n",
    "        return 'Fuse'\n",
    "    elif re.search(r'\\brecloser\\b', desc):\n",
    "        return 'Recloser' \n",
    "    elif re.search(r'\\bswitch\\b', desc):\n",
    "        return 'Switch'\n",
    "    elif re.search(r'\\bload\\b', desc):\n",
    "        return 'Load'\n",
    "    elif re.search(r'\\bbreaker\\b', desc):\n",
    "        return 'Breaker'\n",
    "    else:\n",
    "        return 'Ambiguous'\n",
    "\n",
    "# Apply to rows with \"0\" device type\n",
    "zero_mask = incident_df['Device Type_imputed'].astype(str).str.strip() == \"0\"\n",
    "incident_df.loc[zero_mask, 'Device Type_imputed'] = incident_df.loc[zero_mask, 'Equip Desc'].apply(map_zero_device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Final Class Mapping\n",
    "# ======================\n",
    "# Define authoritative categories\n",
    "MAIN_CATEGORIES = ['Load', 'Fuse', 'Recloser', 'Switch', 'Breaker']\n",
    "\n",
    "def categorize_device(x):\n",
    "    \"\"\"Smart categorization with validation\"\"\"\n",
    "    if x in MAIN_CATEGORIES:\n",
    "        return x\n",
    "    elif x == 'Ambiguous':\n",
    "        # Merge ambiguous cases with smallest existing category\n",
    "        class_counts = incident_df['Device Type_imputed'].value_counts()\n",
    "        smallest_class = class_counts[class_counts.index.isin(MAIN_CATEGORIES)].idxmin()\n",
    "        return smallest_class\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "incident_df['Device_Final'] = incident_df['Device Type_imputed'].apply(categorize_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 5. Validation & Quality Checks\n",
    "# ======================\n",
    "print(\"\\n=== Class Distribution ===\")\n",
    "dist = incident_df['Device_Final'].value_counts(normalize=True)\n",
    "display(dist)\n",
    "\n",
    "print(\"\\n=== Imputation Accuracy Check ===\")\n",
    "# Check original vs imputed values where both exist\n",
    "valid_mask = incident_df['Device Type'].notna() & incident_df['Device_Final'].notna()\n",
    "accuracy = (incident_df.loc[valid_mask, 'Device Type'] == incident_df.loc[valid_mask, 'Device_Final']).mean()\n",
    "print(f\"Consistency with original data: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 6. Final Output\n",
    "# ======================\n",
    "label_map = {cat: idx for idx, cat in enumerate(MAIN_CATEGORIES)}\n",
    "incident_df['Device_Label'] = incident_df['Device_Final'].map(label_map)\n",
    "\n",
    "# Save processed data\n",
    "incident_df.to_excel('Incident_multi_class_processed.xlsx', index=False)\n",
    "print(\"\\nProcessing complete. Final labels:\")\n",
    "print(incident_df['Device_Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# ======================\n",
    "# 1. Data Loading\n",
    "# ======================\n",
    "incident_df = pd.read_excel('Incidents_imputed.xlsx', engine='openpyxl')\n",
    "print(\"Initial data shape:\", incident_df.shape)\n",
    "\n",
    "# ======================\n",
    "# 2. Unique Values & Distribution for Equip Desc\n",
    "# ======================\n",
    "# Get frequency distribution for Equip Desc\n",
    "equip_desc_counts = incident_df['Equip Desc'].value_counts()\n",
    "print(\"Number of unique Equip Desc values:\", equip_desc_counts.shape[0])\n",
    "print(\"\\nEquip Desc Distribution:\")\n",
    "display(equip_desc_counts)\n",
    "\n",
    "# Optionally, display the sorted list of unique Equip Desc values for inspection:\n",
    "unique_equip_desc = sorted(incident_df['Equip Desc'].dropna().unique())\n",
    "print(\"\\nSorted list of unique Equip Desc values:\")\n",
    "display(unique_equip_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms in 'Miscellaneous':\n",
      "Equip Desc\n",
      "CUSTOMER EQUIPMENT       22818\n",
      "SUBSTATION CIRCUIT       11790\n",
      "OTHER                    11364\n",
      "POLE                     11071\n",
      "CONNECTOR                 6482\n",
      "JUMPER                    5907\n",
      "METER - SECONDARY         1935\n",
      "CROSSARM                  1782\n",
      "LINE RECLOSER             1408\n",
      "ARRESTOR OH               1392\n",
      "INSULATOR                 1282\n",
      "SCADA - CIRCUIT TRIP      1188\n",
      "SCADA - LOCKOUT           1111\n",
      "PEDESTAL/J-BOX             910\n",
      "PIN                        525\n",
      "BUSHING                    294\n",
      "CUSTOMER CONNECTIVITY      180\n",
      "ELBOW                      169\n",
      "METER - PRIMARY            160\n",
      "SPLICE - OH                139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "misc_df = incident_df[incident_df['Equipment_Group'] == 'Miscellaneous']  \n",
    "print(\"Top Terms in 'Miscellaneous':\")  \n",
    "print(misc_df['Equip Desc'].value_counts().head(20))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Class Distribution:\n",
      "Equipment_Group\n",
      "Fuse                  0.425505\n",
      "Conductor             0.207312\n",
      "Customer_Equipment    0.099457\n",
      "Other                 0.090955\n",
      "Infrastructure        0.059268\n",
      "Connector             0.054395\n",
      "Transformer           0.050524\n",
      "Protection_Device     0.012585\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "incident_df = pd.read_excel('Incidents_imputed.xlsx', engine='openpyxl')\n",
    "\n",
    "# Step 1: Remove noise\n",
    "noise_terms = ['CANCELLED', 'ON UPON ARRIVAL', 'OTHER']\n",
    "incident_df = incident_df[~incident_df['Equip Desc'].isin(noise_terms)]\n",
    "\n",
    "# Step 2: Enhanced functional grouping\n",
    "EQUIP_GROUP_MAPPING = {\n",
    "    # Core Electrical Components\n",
    "    r'\\bFUSE\\b': 'Fuse',\n",
    "    r'\\bCUTOUT\\b': 'Fuse',\n",
    "    r'\\b(RECLOSER|ARRESTER|ARRESTOR)\\b': 'Protection_Device',\n",
    "    r'\\bTRANSFORMER\\b': 'Transformer',\n",
    "    \n",
    "    # Conductors & Connections\n",
    "    r'\\bCONDUCTOR\\b': 'Conductor',\n",
    "    r'\\b(SPLICE|CONNECTOR|JUMPER)\\b': 'Connector',\n",
    "    \n",
    "    # Structural Components\n",
    "    r'\\b(POLE|CROSSARM|PIN|BUSHING)\\b': 'Structural',\n",
    "    \n",
    "    # Customer-Related\n",
    "    r'\\bCUSTOMER\\b': 'Customer_Equipment',\n",
    "    \n",
    "    # Monitoring & Control\n",
    "    r'\\bSCADA\\b': 'Monitoring_System',\n",
    "    r'\\bMETER\\b': 'Meter',\n",
    "    \n",
    "    # Power Regulation\n",
    "    r'\\bREGULATOR\\b': 'Voltage_Regulator',\n",
    "    r'\\bCAPACITOR\\b': 'Power_Factor_Correction'\n",
    "}\n",
    "\n",
    "def categorize_equip(desc):\n",
    "    desc = str(desc).upper()\n",
    "    for pattern, group in EQUIP_GROUP_MAPPING.items():\n",
    "        if re.search(pattern, desc, flags=re.IGNORECASE):\n",
    "            return group\n",
    "    return 'Other'\n",
    "\n",
    "incident_df['Equipment_Group'] = incident_df['Equip Desc'].apply(categorize_equip)\n",
    "\n",
    "# Step 3: Strategic merging of small groups\n",
    "incident_df['Equipment_Group'] = incident_df['Equipment_Group'].replace({\n",
    "    'Voltage_Regulator': 'Power_Management',\n",
    "    'Power_Factor_Correction': 'Power_Management',\n",
    "    'Meter': 'Monitoring_System',\n",
    "    'Structural': 'Infrastructure'\n",
    "})\n",
    "\n",
    "# Final class consolidation\n",
    "incident_df['Equipment_Group'] = incident_df['Equipment_Group'].replace({\n",
    "    'Power_Management': 'Other',          # 2.8% → Other\n",
    "    'Monitoring_System': 'Other'          # 1.9% → Other\n",
    "})\n",
    "\n",
    "# Verify distribution\n",
    "print(\"\\nFinal Class Distribution:\")\n",
    "dist = incident_df['Equipment_Group'].value_counts(normalize=True)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Distribution:\n",
      "Equipment_Group\n",
      "Fuse                  0.425505\n",
      "Conductor             0.207312\n",
      "Infrastructure        0.164330\n",
      "Customer_Equipment    0.139265\n",
      "Protection_Device     0.063589\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import re  \n",
    "\n",
    "# Load data  \n",
    "incident_df = pd.read_excel('Incidents_imputed.xlsx', engine='openpyxl')  \n",
    "\n",
    "# Step 1: Remove noise  \n",
    "noise_terms = ['CANCELLED', 'ON UPON ARRIVAL', 'OTHER']  \n",
    "incident_df = incident_df[~incident_df['Equip Desc'].isin(noise_terms)]  \n",
    "\n",
    "# Step 2: Enhanced regex patterns  \n",
    "EQUIP_GROUP_MAPPING = {  \n",
    "    r'\\bFUSE\\b': 'Fuse',  \n",
    "    r'\\bCUTOUT\\b': 'Fuse',  \n",
    "    r'\\bCONDUCTOR\\b': 'Conductor',  \n",
    "    r'\\b(RECLOSER|ARRESTOR|SUBSTATION CIRCUIT|RELAY|CIRCUIT BREAKER|GROUNDING)\\b': 'Protection_Device',  \n",
    "    r'\\b(TRANSFORMER|XFMR)\\b': 'Transformer',  \n",
    "    r'\\b(CONNECTOR|JUMPER|SPLICE)\\b': 'Infrastructure',  \n",
    "    r'\\bCUSTOMER\\b': 'Customer_Equipment',  \n",
    "    r'\\b(POLE|CROSSARM|PIN|TOWER|ANCHOR)\\b': 'Infrastructure',  \n",
    "    r'\\b(REGULATOR|CAPACITOR)\\b': 'Power_Management'  \n",
    "}  \n",
    "\n",
    "def categorize_equip(desc):  \n",
    "    desc = str(desc).upper()  \n",
    "    for pattern, group in EQUIP_GROUP_MAPPING.items():  \n",
    "        if re.search(pattern, desc, flags=re.IGNORECASE):  \n",
    "            return group  \n",
    "    return 'Customer_Equipment'  # Default to customer-facing issues  \n",
    "\n",
    "incident_df['Equipment_Group'] = incident_df['Equip Desc'].apply(categorize_equip)  \n",
    "\n",
    "# Step 3: Final merging  \n",
    "incident_df['Equipment_Group'] = incident_df['Equipment_Group'].replace({  \n",
    "    'Power_Management': 'Infrastructure',  \n",
    "    'Transformer': 'Infrastructure'  \n",
    "})  \n",
    "\n",
    "# Verify  \n",
    "print(\"\\nFinal Distribution:\")  \n",
    "print(incident_df['Equipment_Group'].value_counts(normalize=True))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
